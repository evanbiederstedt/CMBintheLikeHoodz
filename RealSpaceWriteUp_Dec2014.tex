%%    TEMPLATE for articles submitted to the full-tex econf proceedings
%%     
%%
%%     Please do not remove lines commented out with %+
%%           these are for the editors' use.
%%
%%     Questions?  Send email to :  mpeskin@slac.stanford.edu
%%   

\documentclass[12pt]{article}
\usepackage{epsfig}
\usepackage{amsfonts}
\usepackage{amsmath}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Do not change these:
\textwidth=6.0in  \textheight=8.25in

%%  Adjust these for your printer:
\leftmargin=-0.3in   \topmargin=-0.20in


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  personal abbreviations and macros
%    the following package contains macros used in this document:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  To include an item in the INDEX of the conference volume,
%           flag it with    \index{<item name>}
%  The use of this macro is illustrated in the text.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\Title#1{\begin{center} {\Large {\bf #1} } \end{center}}

\begin{document}


\Title{Methodology for CMB Angular Power Spectrum Extraction}

\bigskip

%+\addtocontents{toc}{{\it D. Reggiano}}
%+\label{ReggianoStart}

\begin{raggedright}  


Evan Biederstedt\\
December 18, 2014
\bigskip
\end{raggedright}





\section{Introduction}

The observations from astrophysical surveys often result in signals and images containing contributions from several components or sources. This is particularly troublesome for cosmic microwave background (CMB) experiments, where the CMB signal is mixed with various astrophysical emissions (both galactic and extragalactic) and noise. As a result, much effort has been concentrated in the development of techniques to isolate each emission from all the other components present in data, including unwanted components (e.g. detector noise). The extraction of the CMB signal from the set of multifrequency observations includes getting the best possible map of the CMB with the least possible foreground contamination, but also achieving the best possible estimate of the CMB angular power spectrum, $C_l$. This short exposition will concentrate on the latter. The usual approach to this problem is to calculate unbiased estimators of the angular power spectrum $C_l$ from the values of sky maps, with many techniques attempting to exploit the sparsity of the CMB power spectrum in different dictionaries (e.g. wavelets, needlets, etc.). We detail a new data-based method for extracting the true underlying CMB angular power spectrum, with the aim of developing similar techniques to extract other emissions in the maps developed by the Planck collaboration in the future.

Algorithms for statistical analysis has been very well-developed for the linear domain. However, data in the "real world" require nonlinear methods to detect and model the kind of dependencies that allow successful prediction of properties of interest. By utilizing the strong theoretical and empirical priors of the CMB power spectrum, our goal is to model the power spectrum $C_l$ with a Gaussian process, a type of fully probabilistic Bayesian model to do nonparametric regression.

Furthermore, recall that the sky maps produced by CMB surveys like WMAP and Planck are just one realization of the underlying true angular power spectrum. Due to the limit imposed by cosmic variance, it is impossible to recover "the perfect" accurate CMB power spectrum. Due to this fundamental uncertainty in estimating the underlying $C_l$, this problem seems poised for Bayesian methods.

\section{CMB Data Analysis}


Much previous work in CMB data analysis has been focused on model testing and parameter estimation, e.g. assuming a theoretical model and then deciding if the best fit parameter values describe the data well. For cosmological purposes, the four main steps to the data analysis is (1) clean the timestream of raw data to produce time-ordered data (TOD), (2) use the TOD to create maps, (3) estimate the power spectrum from these maps, and then (4) derive cosmological parameters.

The Planck mission measures the entire sky at nine frequency bands covering $30-857$ GHz, with an unprecedented angular resolution ($5'$ arcmin) and sensitivity $(\Delta T/T\sim2\times10^{-6})$. This will allow us to measure the CMB angular power spectrum to the highest accuracy yet, as well as exploring the properties of astrophysical phenomena inside and outside our galaxy. 




Cosmological theories really only predict statistical properties of the universe. As a result, we often rely upon the definition of a power spectrum $P(k)$ to describe how much some field varies on different scales. Let's begin by considering the power spectrum for the distribution of galaxies, characterized by the inhomogeneities of the field density labeled $\delta(\vec{x})$, we can write the power spectrum $P(k)$ in the form

\begin{equation}
\langle\vec{\delta}(\vec{k})\vec{\delta}(\vec{k'})\rangle=(2\pi)^3P(k)\delta^{3}(\vec{k}-\vec{k}^{'})
\end{equation}

where $\delta(\vec{k})$ denotes the Fourier transform of $\delta(\vec{x})$. The power spectrum then is the variance in the distribution, e.g. for a very smooth distribution, $P(k)$ is small, and if there are many underdense/overdense regions in our field, the power spectrum $P(k)$ will be very large. Note the dimensions of the power spectrum is (length$)^3$, i.e. $(k)^{-3}$. Therefore, the dimensionless value $k^3P(k)/2\pi^2$ indicates the "clumpiness" on scale $k$. 

CMB surveys however result in a temperature measured via two angular coordinates on the celestial, a two-dimensional field. So, one usually does not Fourier transform the CMB temperature, but expands the two-dimensional field on the surface of a sphere via spherical harmonics as a function of multipole moment $l$, not wave number $k$. Moreover, there is an angular variation in the temperature of the sky when observing the CMB. That is, our maps of the CMB do not show absolute temperature, but rather anisotropies, differences between measurements taken in different directions. By calculating the CMB angular power spectrum, you get the the power at a given angular scale---depending on the angular resolution you use, you will see a smaller/bigger contrast. We usually plot the angular power spectrum as how much temperature varies from point to point on the sky $l(l+1)C_l/2\pi$ (in units microKelvin squared, ($\mu K)^2$) versus the angular frequency, multipole $l$. Here the value of the multipole $l$ is given by 

\begin{equation}
l\simeq\frac{\pi}{\theta}
\end{equation}

where $\theta$ is the angle a certain CMB survey feature subtends in the sky. 


The distribution of matter and curvature of $\Lambda$CDM seems to be consistent with small fluctuations  produced by a statistically isotropic Gaussian random process. In addition, much theoretical work also motivates the conclusion that initial fluctuations of the CMB form a Gaussian random field, namely inflation.
Therefore, in the case of the Gaussian-distributed fluctuations, the angular power spectrum $C_l$ contains virtually all the statistical information. The angular power spectrum $C_l$ is the variance of the spherical harmonic coefficients $a_{lm}$, corrected for beam smearing. It represents the fluctuation power at some given angular scale, i.e. the amplitude of CMB fluctuations at said given angular extension on the sky. 

For the spherical harmonic coefficients, we cannot make predictions about any particular $a_{lm}$, we can only make predictions about the distribution from which they are drawn. Recall the mean is $\langle a_{lm}\rangle=0$ and the variance is written as $\langle a_{lm} a^{*}_{l'm'}\rangle=\delta_{ll'}\delta_{mm'}C_l$. So, for a given $l$, each $a_{lm}$ has the same variance.

By utilizing angular transfer function $T(k,l)$, we can calculate the primordial power spectrum $P(k)$ from our calculated angular power spectrum $C_l$ via


\begin{equation}
C_l=4\pi\int\frac{dk}{k}T^2(k,l)P(k)
\end{equation}

Note that this spectrum $P(k)$ depends on the cosmological parameters via scale $k$ and is used to determine said cosmological parameters. 



\section{The Angular Power Spectrum}

We begin by focusing on how the CMB angular power spectrum $C_l$ is usually computed:

The most common pixelization scheme for CMB data analysis is based on HEALPix, the "Hierarchical, Equal Area, isoLatitutde Pixelization" scheme. Using HEALPix, pixels of equal area are organized in a hierarchical system with respect to resolution, with pixel centers lying on the rings of constant latitude. These properties are essential for algorithmic operations such as nearest-neighbor searches, Fast Fourier Transforms (FFTs) on our maps and fast numerical integration on the sphere.

We can represent the fluctuations over the sky in terms of spherical harmonics,

\begin{equation}
\frac{\Delta T}{T}(\theta, \phi) = \sum a_{lm}Y_{lm}(\theta, \phi)
\end{equation}

 i.e we can separate out the contributions of different angular scales by doing multipole expansion. For the purpose of data analysis, we often include $W_l$, the window function of a user-defined FWHM (full-width, half-maximum), 

\begin{equation}
\frac{\Delta T}{T}(\theta, \phi) = \sum a_{lm}W_lY_{lm}(\theta, \phi)
\end{equation}


The beam window function is defined as $W_l=\exp\big[-l(l+1)/(2l^2_{\mbox{beam}}\big]$ for a Gaussian beam with $l_{\mbox{beam}}=\sqrt{8\ln2}(\theta_{\mbox{beam}})^{-1}$.

Note that for spherical harmonics, the functions are orthonormal on the sphere as usual,

\begin{equation}
\int d\Omega\  Y_{lm}(\theta, \phi)Y^{*}_{l'm'}(\theta, \phi)=\delta_{ll'}\delta_{mm'}
\end{equation}.


Pixelating $\Delta T/T(\theta, \phi)$ corresponds to sampling it at $N_{\mbox{pix}}$ locations $(\theta_p, \phi_p)$, for $p\in[0, N_{\mbox{pix}}-1]$. The sample function values at  $\Delta T/T_p$ are then used to estimate $a_{lm}$,

\begin{equation}
\hat{a}_{lm}=\frac{4\pi}{N_{\mbox{pix}}}\sum\limits^{ N_{\mbox{pix}}-1}_{p=0}Y^{*}_{lm}(\theta_p, \phi_p)\frac{\Delta T}{T}(\theta_p, \phi_p)
\end{equation}

Using these estimated $a_{lm}$ coefficients, we then calculate estimators $\hat{C}_l$ ,

\begin{equation}
\hat{C}_l = \frac{1}{2l+1}\sum\limits^{l}_{m=-l}a_{lm}a^{*}_{lm}=\frac{1}{2l+1}\sum\limits^{l}_{m}\|\hat{a}_{lm}\|^2
\end{equation}


at each $l$ value. In summary, we use the spherical harmonic domain of the pixelized maps, estimate the values of the spherical harmonic coefficients $a_{lm}$, and use these values to calculate the unbiased estimator $\hat{C}_l$ for all values of multipole $l$, and then plot the entire angular power spectrum, temperature vs. multipole.




\section{The Angular Correlation Function}

Though we usually view CMB temperature fluctuations through spherical harmonics, we can also use a two-point correlation function to express the correlation between temperature perturbations at two points, $i$ and $j$ separated by some angle $\theta_{ij}$. We express the relationship between the angular power spectrum and the angular correlation function as follows:


The two-point correlation function 

\begin{equation}
C\big(\theta_{ij}\big)=\langle \frac{\Delta T}{T}(\hat{n}_i)\frac{\Delta T}{T}(\hat{n}_j)\rangle
\end{equation}

can be viewed as in Fourier space angle on the sky is the CMB angular power spectrum. Note that $\langle\dots\rangle$ represents the average performed over all points at $\hat{n}_i$ and $\hat{n}_j$ separated by the angle $\theta_{ij}=|\hat{n}_i-\hat{n}_j|$ over the sphere. 

Then, the angular power spectrum is related to the angular two-point correlation function by the following derivation:


\begin{eqnarray*}
C(\theta_{ij}) & = & \langle \frac{\Delta T}{T}(\hat{n}_i)\frac{\Delta T}{T}(\hat{n}_j)\rangle \\
& = & \sum\limits_{lm}\sum\limits_{l'm'}a_{lm}a_{l'm'}^{*}Y_{lm}(\hat{n}_i)Y_{l'm'}(\hat{n}_j)
=\frac{1}{4\pi}\sum\limits_{l}(2l+1)C_l P_l (\cos\theta_{ij})
\end{eqnarray*}



where $P_l$ denotes the Legendre polynomial of order $l$,  $\cos\theta_{ij}=\hat{n}_i\cdot\hat{n}_j$
and $\theta_{ij}$ is the angle between pixels at position $\hat{n}_i$ and $\hat{n}_j$. 

Note we relied upon the "close relation" of spherical harmonic bases $Y_{lm}$, i.e. by summing over the $m$ corresponding to the same multipole number $l$ we find

\begin{equation}
\sum_m\|Y_{lm}(\theta, \phi)\|^2=\frac{2l+1}{4\pi}
\end{equation}



\section{The Likelihood Function}
The first step of probabilistic inference is to write down the likelihood function. Let's say we wish to estimate the angular power spectrum $C_l$ by explicitly maximizing the likelihood function. Working in the domain of pixels, the temperature values of observed pixels should be described as random variables following a Gaussian distribution. If the temperature fluctuations are Gaussian, then the likelihood function Pr(data $\rvert$ theory) is expressed as a multivariate Gaussian 

\begin{equation}
\mathcal{L}=\Pr(\textbf{d}\rvert C_l)=\frac{1}{(2\pi)^{N_{pix}/2}(\det\textbf{C})^{1/2}}\exp{\Big(-\frac{1}{2}\textbf{d}^{T}\textbf{C}^{-1}\textbf{d}\Big)}
\end{equation}

where our data vector $\textbf{d}$ is the vector of noisy temperature map measurements $\textbf{T}_i$ at a set of pixel locations $\hat{n}_i$, and our covariance matrix $\textbf{C}$ is an $n\times n$ positive, symmetric matrix. The evaluation of the matrix inversion $\textbf{C}^{-1}$ and the determinant $\det{|\textbf{C}|}$ requires a computational complexity of $\mathcal{O}(n^3)$, which for large values $n$ is computationally intractable. (In our case, we are taking $n$ to be the number of pixels on a sky map, $N_{pix}$). We often expect the covariance matrix to be in the form $\textbf{C}=\sigma^2\mathbb{I}+\textbf{K}$ where (as we explain below) we can express $K_{ij}$ in terms of a kernel matrix, $K_{ij}=k(x_i, x_j)$. 

Let's take the covariance matrix to be the superposition of the signal part $\textbf{S}$ as a function of the power spectrum $C^{theory}_l$ and the instrumental noise term, i.e. $\textbf{C}=\textbf{S}+\textbf{N}$. 

Our pixel-space noise covariance matrix will be written as 

\begin{equation}
C_{ij}=N_{ij}+S_{ij}=N_{ij}+\frac{1}{4\pi}\sum\limits_{l}(2l+1)C^{theory}_l P_l (\hat{n}_i\cdot\hat{n}_j)
\end{equation}

where the noise matrix is nearly diagonal and written as

\begin{equation}
N_{ij}=\sigma^2_i\delta_{ij}
\end{equation}

where $\sigma_i^2$ is the rms noise in pixel $i$, \cite{SpergelOh}, \cite{Hinshaw}. Similarly, the signal matrix $S_{ij}$ is simply the angular correlation function between two pixels, and is composed of off-diagonal components generated by the cosmological model. As we show below, this covariance function $S_{ij}$ relating the similarity between pixels at position $\hat{n}_i$ and $\hat{n}_j$ will be an example of a kernel (in fact, a Mercer kernel). Indeed, our ability to "substitute" $S_{ij}$ as a kernel in place of $K_{ij}$ is an example of the "kernel trick". 



Assuming noise equals zero, we could write the likelihood for temperature fluctuations in full sky coverage as 

\begin{equation}
P(\textbf{T}\rvert C^{theory}_l)\propto\frac{\exp\big(-(\textbf{T}S^{-1}\textbf{T})/2\big)}{\sqrt{\det S}}
\end{equation}

where $\textbf{T}$ is the temperature map and $S_{ij}=\Sigma_l (2l+1)C^{theory}_lP_l(\hat{n}_i\cdot\hat{n}_j)/4\pi$ with Legendre polynomials $P_l$ and the pixel position on the map $\hat{n}_i$. 

Covariance matrices of the form above are often found in Gaussian process models, which we will introduce below. 

%Expressing the equation in terms of the log-likelihood, we can then maximize the likelihood by solving

%\begin{equation}
%\frac{\partial}{\partial C_l}(-2 \ln\mathcal{L})=\textbf{d}^{T}\textbf{C}^{-1}\frac{\partial\textbf{C}}{\partial %C_l}\textbf{C}^{-1}\textbf{d}+\mbox{Tr}\big(\textbf{C}^{-1}\frac{\partial\textbf{C}}{\partial C_l}\big)=0
%\end{equation}

\section{Inference Properties of Gaussians}
Except sums of independent and identically distributed (i.i.d.) random variables, nothing in the real world is actually Gaussian. However, it can be argued that inasmuch as linear maps provide the natural language for algebra, Gaussians provide a distinct language for inference. Consider the multivariate Gaussian distribution: for mean vector $\mu$ and covariance matrix $\Sigma$, the joint probability density is written as

\begin{equation}
\mathcal{N}(\textbf{x};\mu,\Sigma)=\frac{1}{(2\pi)^{N/2}|\Sigma|^{1/2}}\exp\Bigg[-\frac{1}{2}(\textbf{x}-\mu)^{T}\Sigma^{-1}(\textbf{x}-\mu)\Bigg]
\end{equation}

where $\textbf{x}, \mu\in\mathbb{R}^{N}$ and $\Sigma\in\mathbb{R}^{N\times N}$. The covariance matrix $\Sigma$ is positive semidefinite, i.e. it is Hermitian for all eigenvalues greater than zero, and $\textbf{v}^{T}\Sigma\textbf{v}$ is greater than zero for any N-dimensional vector $\textbf{v}$. %$\forall \ \textbf{v}\in\mathbb{R}^{N}$. 

The properties of this distribution are varied and flexible enough to do probabilistic inference:

\textbf{Property 1:} Gaussians are closed under multiplication,

\begin{eqnarray*}
\mathcal{N}(\textbf{x}; a, A)\mathcal{N}(\textbf{x}; b, B) & = & \mathcal{N}(\textbf{x}; c, C)\mathcal{N}(a; b, A+B) \\
            \mbox{where} \ C\equiv(A^{-1}+B^{-1}) & \mbox{and} & c\equiv C(A^{-1}a+B^{-1}b).
\end{eqnarray*}

 i.e. the product of two Gaussians results in another un-normalized Gaussian.

\textbf{Property 2:} Gaussians are also closed under linear maps, i.e. linear maps of Gaussians are Gaussians

\begin{eqnarray*}
p(z) & = & \mathcal{N}(z; \mu, \Sigma) \\
           \implies p(\textbf{A}z) & = & \mathcal{N}(\textbf{A}z, \textbf{A}\mu, \textbf{A}\Sigma\textbf{A}^{T})
\end{eqnarray*}
for some vector $\textbf{A}$.

\textbf{Property 3:} Gaussians are closed under marginalization as well. For jointly Gaussian vectors $\textbf{x}$ and $\textbf{y}$, we use the notation

\[
\begin{bmatrix}
  \textbf{x} \\  \textbf{y}
\end{bmatrix}
\sim\mathcal{N}
\begin{pmatrix}\begin{bmatrix}
  \mu_x \\ \mu_y
\end{bmatrix}, 
\begin{bmatrix}
\Sigma_{xx} & \Sigma_{xy} \\ \Sigma_{yx} & \Sigma_{yy} 
\end{bmatrix}\end{pmatrix}
\]

The marginal distribution of $\textbf{x}$ is therefore written as
\[
\int\mathcal{N}
\begin{pmatrix}\begin{bmatrix}
  \mu_x \\ \mu_y
\end{bmatrix}, 
\begin{bmatrix}
\Sigma_{xx} & \Sigma_{xy} \\ \Sigma_{yx} & \Sigma_{yy} 
\end{bmatrix}\end{pmatrix}\mbox{d}y
=\mathcal{N}(x;\mu_x, \Sigma_{xx})
\]


From this we can conclude that every finite-dimensional Gaussian is marginal of infinitely many more Gaussians. 

\textbf{Property 4:} Gaussians are also closed under conditioning. 

\begin{equation}
\textbf{x} | \textbf{y}\sim\mathcal{N}\big(\textbf{x}; \mu_x+\Sigma_{xy}\Sigma^{-1}_{yy}(\textbf{y}-\mu_y), \ \Sigma_{xx}-\Sigma_{xy}\Sigma^{-1}_{yy}\Sigma_{yx}\big)
\end{equation}

Let's re-examine the last two properties, 3 and 4. On closer examination, the third property reveals itself as a form of the "Sum Rule"

\begin{equation}
\int p(x,y)\mbox{d}y=\int p(y|x)p(x)\mbox{d}y=p(x)
\end{equation}

which provides a definition of marginal probability. That is, marginal probabilities are sums of join probabilities integrating out all random variables except the target random variables. Furthermore, the fourth property depends on a form of the "Product Rule" 

\begin{equation}
p(x,y)=p(x|y)p(y)
\end{equation}

That is, joint probabilities are the products of conditional and marginal probabilities. The conditional form of "Property 4" above is written as
\begin{equation}
p(x|y)=\frac{p(x,y)}{p(y)}=\mathcal{N}\big(\textbf{x}; \mu_x+\Sigma_{xy}\Sigma^{-1}_{yy}(\textbf{y}-\mu_y), \ \Sigma_{xx}-\Sigma_{xy}\Sigma^{-1}_{yy}\Sigma_{yx}\big)
\end{equation}. 

Therefore, we have shown that Gaussians are in fact closed under all the rules of probability. And as Baye's Rule is simply a consequence of the Sum and Product Rules, we can do Bayesian inference by focusing on processes which are Gaussian.


\section{Kernels}

Often times in probabilistic inference, it is difficult to know how to represent certain kinds of inputs as fixed-size feature vectors, $x_i\in\mathbb{R}^{D}$. There are several approaches to this difficulty in the machine learning literature, but one of the most fruitful approaches is to measure the similarity between objects without somehow preprocessing these objects as fixed-sized feature vectors. We use a "kernel function"  $\kappa(x,x')$ as the similarity measure between pairs of data points. The kernel is a dot product in a (usually high dimensional) feature space. In this space, our estimation methods are usually linear, but as long as we can formulate everything in terms of kernel evaluations, we never explicitly have to compute in the high-dimensional feature space.

For inputs $x$ and $x'$ in some input space $\mathcal{X}$, the kernel function $\kappa(x,x')\in\mathbb{R}$ can express these arguments as an inner product in another space $\mathcal{V}$. (We normally require this function to be both non-negative $\kappa(x,x')\geq0$ and symmetric  $\kappa(x,x')=\kappa(x',x)$.) For a particular inference problem, we simply construct some "feature space" mapping $\phi: \mathcal{X}\rightarrow\mathcal{V}$ such that the kernel function is expressed as

\begin{equation}
\kappa(x,x')=\langle\phi(x), \phi(x')\rangle
\end{equation}

given $\langle \ \cdot \ ,\ \cdot \ \rangle$ is an inner product in feature space $\mathcal{V}$.

For a given kernel $\kappa$ with inputs $x_1,\cdots,x_n\in\mathcal{X}$, the $n\times n$ matrix 

\begin{equation}
\textbf{K}\equiv\big(\kappa(x_i, x_j)\big)_{ij}=\begin{pmatrix}
\kappa(x_1, x_1) & \cdots & \kappa(x_1, x_N) \\ 
\vdots & \ddots & \vdots\\
\kappa(x_N, x_1) & \cdots & \kappa(x_N, x_N)\end{pmatrix}
\end{equation}

is referred to as a Gram matrix. If $K_{ij}$ is a real $n\times n$ symmetric matrix of the form 

\begin{equation}
\sum\limits_{i,j}c_ic_jK_{ij}\geq0
\end{equation}

for all $c_i\in\mathbb{R}$, then the matrix $K_{ij}$ is positive semidefinite.

If a Gram matrix is positive semidefinite, we refer to this as a Mercer kernel. The significance of a Mercer kernel is a result of Mercer's theorem. This states that if the kernel is Mercer, then there exists a function $\phi$ mapping $x\in\mathcal{X}$ to (potentially) some infinite dimensional space $\mathbb{R}^D$ such that

\begin{equation}
\kappa(x,x')=\phi(x)^{T}\phi(x')
\end{equation}

where the mapping $\phi$ depends on the eigenfunctions of the kernel $\kappa$. To be more explicit, consider a positive semidefinite Gram matrix $\textbf{K}$. We can write the eigenvector decomposition of this matrix as

\begin{equation}
\textbf{K}=\textbf{U}^{T}\Lambda\textbf{U}
\end{equation}

such that $\Lambda$ is a diagonal matrix with eigenvalues $\lambda_i>0$. If we focus on one element of the matrix $\textbf{K}$,

\begin{equation}
k_{ij}=(\Lambda^{\frac{1}{2}}\textbf{U}_{:,i})^{T}(\Lambda^{\frac{1}{2}}\textbf{U}_{:,j})
\end{equation}

we can define the mapping as $\phi(x_i)=\Lambda^{\frac{1}{2}}\textbf{U}_{:,i}$, which gives us 

\begin{equation}
k_{ij}=\phi(x_i)^{T}\phi(x_j)
\end{equation}.

The importance of this property becomes apparent when we specifically construct new kernel functions to describe our priors; new Mercer kernels can be constructed from simpler Mercer kernels via a set of standard operations (e.g. the linear combination of two Mercer kernels is in fact a Mercer kernel.) As many methods require the covariance matrix to be positive semidefinite, proving your kernel is satisfies the Mercer theorem shows it is well-defined. (For a full account of constructing kernels, see \cite{Schoelkopf}.)

%Viewing a Gaussian process as priors directly on the "function space" with a specified kernel function %suitable for the problem at hand. 

By deciding on a specific kernel, we are also choosing both the measure of similarity and the representation for our data. This is quite powerful, as it turns out many algorithms defined in terms of inner products in input space become tractable if we project inputs into this implicit higher-dimensional feature space and carry out computations there. 

Furthermore, by constructing an appropriate kernel, we are choosing the covariance function for correlated observations. That is, if we have some knowledge about the relationship of our inputs of our problem (or more specifically, how our observations at different points relate to each other), we can encode this prior knowledge through our kernel. Indeed, one can think of our choice of kernel as the prior knowledge we have about a problem and its solution.

In the case of Gaussian processes, this insight is very strong, as the kernel we construct establishes how likely different functions are considered a priori. That is, we are determining the prior over an entire set of functions. In a nutshell, this is what a Gaussian process does: it defines prior distributions on functions, allowing one to perform Bayesian nonparametric regression. (By the term "nonparametric", we mean that the complexity of the solution increases with the amount of data inputs.)




\section{Gaussian Process}

As a conceptual crutch, think of a Gaussian process as a continuous stochastic process which directly defines a prior probability distribution over functions, i.e. an uncountably infinite set of functions. Once encountering some finite set of data, this is converted into a posterior over functions. The more data we see, the stronger this posterior becomes. 

Consider a random set of variables which are indexed by some continuous variable, i.e. a function $f(x)$. Some finite subset of inputs $\textbf{X}=\{x_1, x_2, \cdots, x_N\}$ can be expressed as a subset of random function variables $\textbf{f}=\{f_1, f_2, \cdots, f_N\}$. In a Gaussian process, any such finite set $p(f(x_1), \cdots, f(x_N))$ is jointly Gaussian, i.e. 

\begin{equation}
p(\textbf{f}|\textbf{X})=\mathcal{N}(\mu, \textbf{K})
\end{equation}

with some mean $\mu$ and the covariance matrix given by a positive semidefinte kernel function $\Sigma_{ij}=\kappa(x_i,x_j)$, i.e. a Mercer kernel. Recall via the kernel that if inputs $x_i$ and $x_j$ possess some similarity, the output of the function at those inputs should be similar as well, defining the correlations between different points in the process. This choice of covariance function is vital to inference, as it specifically determines the the type of sample functions drawn from the Gaussian process prior. (That is, the sample functions' shape, smoothness, amplitude, lengthscales, and so on, specified by values of the hyperparameters.) Here, kernels may be conceptualized as infinitely large positive semidefinite matrices. Refer to \cite{Rasmussen} and \cite{HoggForemanMackey} for further details. 

\textbf{Predictions for regression}

When using Gaussian processes for regression problems, we need to take into account of noise on the observed target values, $y = f(x) +\epsilon$, where we take $\epsilon\sim\mathcal{N}(0,\sigma_y^2)$ is independent Gaussian white noise of known variance $\sigma^2_y$. 

For the sake of notational simplicity, let's begin by considering a zero mean Gaussian process prior on the function variables, $p(\textbf{f})=\mathcal{N}(0,\textbf{K})$. The likelihood of the noise is written as $p(\textbf{y}|\textbf{f})=\mathcal{N}(\textbf{f}, \sigma^2\textbf{I})$, where $\textbf{I}$ is the identity matrix. By integrating over the unobserved function variables $\textbf{f}$, we get the marginal likelihood 

\begin{equation}
p(\textbf{y})=\int\mbox{d}\textbf{f}p(\textbf{y}|\textbf{f})p(\textbf{f})=\mathcal{N}(\textbf{0}, \textbf{K}+\sigma_y^2\textbf{I})
\end{equation}


The covariance of the observed noise is then

\begin{equation}
\mbox{cov}[y_i,y_j]=\kappa(x_i,x_j)+\sigma^2_y\delta_{ij}
\end{equation}

(Compare this term to the form of the covariance matrix in Section Five, \\ $\textbf{C}=\sigma^2\mathbb{I}+\textbf{K}$.)

Denoting our prediction outputs as $\textbf{f}_{*}$ with a "test set" $\textbf{X}_{*}$, the joint distribution of the Gaussian process has the form 

\[
\begin{bmatrix}
  \textbf{y} \\  \textbf{f}_{*}
\end{bmatrix}
\sim\mathcal{N}
\begin{pmatrix}\begin{bmatrix}
  \textbf{0} 
\end{bmatrix}, 
\begin{bmatrix}
\textbf{K}_{y} & \textbf{K}_{*} \\ \textbf{K}^{T}_{*} & \textbf{K}_{**} 
\end{bmatrix}\end{pmatrix}
\]

again, using mean equaled to zero for notation. The posterior predictive density is then


\begin{eqnarray*}
p(\textbf{f}_{*}|\textbf{X}_{*}, \textbf{X}, \textbf{y}) & = & \mathcal{N}(\textbf{f}_{*}|\mu_{*}, \Sigma_{*}) \\
\mu_{*} & = & \textbf{K}^{T}_{*}\textbf{K}^{-1}_{y}\textbf{y} \\
\Sigma_{*} & = & \textbf{K}_{**}-\textbf{K}_{*}^{T}\textbf{K}^{-1}_{y}\textbf{K}_{*}.
\end{eqnarray*}

For a single test input, we write this as 

\begin{equation}
p(\textbf{f}_{*}|\textbf{x}_{*}, \textbf{X}, \textbf{y}) = \mathcal{N}(\textbf{f}_{*}|\textbf{k}^{T}_{*}\textbf{K}^{-1}_{y}\textbf{y}, \ k_{**}-\textbf{k}^{T}_{*}\textbf{K}^{-1}_y\textbf{k}_{*})
\end{equation}

where $\textbf{k}_{*}=[\kappa(x_{*}, x_1),\cdots,\kappa(x_{*}, x_{N})]$ and $k_{**}=\kappa(x_{*},x{*})$.

So, we've established that the quality of our predictions for a Gaussian process model depends on the construction of our covariance function. Recall we mentioned that the hyperparamters of the kernel control global properties like lengthscale and amplitude. Detemining the values is key to proper inference. Gaussian processes also allow us to compute hyperparamters from the training data directly; however, it is usually far too complex and/or computationally intractable to place a prior over the hyperparameters themselves and compute the posterior. Hence, we use the marginal likelihood as a cost function. So, in order to estimate these covariance hyperparameters, we simply maximize the marginal likelihood 

\begin{equation}
p(\textbf{y}|\textbf{X})=\int p(\textbf{y}|\textbf{f},\textbf{X})p(\textbf{f}|\textbf{X})\mbox{d}\textbf{f}
\end{equation}

We've noted previously $p(\textbf{f}|\textbf{X})=\mathcal{N}(\textbf{f}|\textbf{0}, \textbf{K})$ and we write the likelihood as $p(y|\textbf{f})=\prod_i\mathcal{N}(y_i|f_i, \sigma^2_y)$. The marginal likelihood is therefore written as

\begin{equation}
\log p(\textbf{y}|\textbf{X})=\log\mathcal{N}(\textbf{y}|\textbf{0}, \textbf{K}_y)=-\frac{1}{2}\textbf{y}\textbf{K}^{-1}_y-\frac{1}{2}\log|\textbf{K}_y|-\frac{N}{2}\log(2\pi)
\end{equation}

To understand this expression, note the first term is the "data fit" term, the second term expresses the model complexity, and the third term is a constant. Varying the values of the hyperparameters affects the tradeoff between the first two terms, e.g. if your fit is very good, the model may be too complex and vice versa.







\section{Employing a Gaussian Process to CMB Sky Maps}

Consider again our likelihood in Section 5, equation (11). We introduced the "signal part" of the covariance matrix $S_{ij}$ to be the angular correlation function between two pixels $i$ and $j$

\begin{equation}
S_{ij}=\frac{1}{4\pi}\sum\limits_{l}(2l+1)C^{theory}_l P_l (\hat{n}_i\cdot\hat{n}_j)
\end{equation}

such that the inputs are the locations of said pixels, at $\hat{n}_i$ and $\hat{n}_j$. We proceeded to claim that this function could serve as a kernel (and hence be used in our Gaussian Process model). Recall the requirements for a kernel $k(x,x')$: it should be symmetric $k(x,x')=k(x',x)$, it should be a Mercer kernel (positive semidefinite symmetric), and it should provide the suitable similarity measure between $x$ and $x'$ for our intended method. $S_{ij}$ takes the inputs of two pixel positions  $\hat{n}_i$ and $\hat{n}_j$, and then computes the angle between the pixels, $\cos\theta_{ij}=\hat{n}_i\cdot\hat{n}_j$. Well, the cosine function is obviously a similarity measure, and it is trivial to see that it is symmetric. Does the cosine satisfy the Mercer matrix condition? Consider a simple dot product kernel, i.e. $k(x,x')=x\cdot x'$. This is perhaps the simplest similarity measure, as we define a mapping $\Phi$ from the input vector $x\in\mathcal{X}$ into our feature space $\mathcal{H}$ such that $\Phi: \mathcal{X}\rightarrow\mathcal{H}$. This allows us to define the similarity measure for the dot product in feature space $\mathcal{H}$

\begin{equation}
k(x,x')=\langle\textbf{x},\textbf{x}'\rangle=\langle\Phi(x),\Phi(x')\rangle
\end{equation}

which is positive semidefinite $k(x,x')>0$ for all $x, x'\in\mathcal{X}$. In terms of the dot product, we can write the cosine kernel as

\begin{equation}
k(x,x')=\frac{x\cdot x'}{|x| |x'|}
\end{equation}

Note that this is simple a scaling of the original dot product. Therefore, we can conclude that the cosine does indeed satisfy the requirements of a kernel. 

As previously mentioned, when constructing kernels, there are several techniques to build new kernels from two individual valid kernels, e.g. a positive constant multiplied by a valid kernel is another valid  kernel, the linear combination of two valid kernels is itself a valid kernel, and the product of two valid kernels is itself a valid kernel. Our covariance matrix $S_{ij}$ also depends on Legendre polynomials. Via Schoelkopf and Smola (2002), a kernel defined on a sphere is positive semidefinite if and only if its expansion into Legendre polynomials $P_l$ only contains nonnegative coefficients, i.e. 

\begin{equation}
k(\xi)=\sum\limits^{\inf}_{l=0}b_lP_l(\xi) \ \mbox{with}\  b_l\geq0
\end{equation}

As the sum of two valid kernels is also a valid kernel, we conclude that our angular correlation function kernel taking in inputs $k(\hat{n}_i, \hat{n}_j)$ is also a valid kernel. 

Having a valid kernel, we can therefore try to extract the true underlying CMB angular power spectrum by fitting a Gaussian Process on all-sky maps. Given the substantial foreground noise, our first step is to use the "cleaned" CMB maps produced by WMAP and/or Planck, and employ a Gaussian Process to calculate the angular power spectrum.

The Planck collaboration has produced a set of "cleaned" temperature and polarization maps of the CMB using several different methods. Commander (based on parametric model fitting in pixel space), NILC (needlet/wavelet-based internal linear combination methods), SMICA (linearly constructed via spectral fitting and filtering), and SEVEM (produced via fitting templates in pixel space). We will begin using cut SEVEM maps with the galactic plane and point sources masked. Using the formalism introduced in Section 3 with the estimators produced by the HEALPix package, we can calculate the values of $C_l$  for each multipole $l$ and plot the results (the usual procedure). We can then use this "estimated" power spectrum as a comparison to check our results. 



\section{Summary and Future Work}
We have detailed a new method to extract the CMB angular power spectrum from all-sky maps produced by CMB surveys based on Gaussian Processes. The next step is to execute similar techniques to isolate astrophysical components in the sky maps produced by Planck data, as well as separate out point sources. Planck is well-positioned to offer full-sky surveys of an extraordinarily wide frequency range of spectral regions which are normally difficult to investigate via ground-based and other more limited surveys.  We shall categorize the astrophysical emissions attainable via Planck as diffuse galactic emission, extragalactic emission, and emission from the solar system. All such emissions are dependent on different multipoles $l$ and frequency $\nu$. 

Diffuse galactic emissions stem from the interstellar medium (ISM) of the milky way. The ISM itself is composed of hot ionized regions, partly ionised regions of intercloud media, and cold diffuse clouds of molecular and atomic gas. The major contributions include free-free radiation, synchrotron emission, thermal dust, and possibly emissions in the microwave range of spinning and/or magnetic dust particles.
Free-free emission results from free electron interacting with ions in an ionised medium, while synchrotron emission is due to charged energetic particles (originating from supernovae shocks) spiraling in a magnetic field. This often generates emission at high galactic latitudes and is therefore less concentrated in the galactic plane than free-free emission or dust. Interstellar dust refers to silicate and carbon-based grains of particulates ranging in size of a few nanometers to a few micrometers. Due to the different sizes and materials, as well as different sources of energy, galactic dust should be present at a large range of temperatures (from around 5 K to over 30 K) and emissivities. Our current understanding of dust is mostly based on extinction observations in the near-IR to UV domain, as well as emission in the radio to IR domains. Dust becomes the dominant "foreground" source above the $70$ GHz range, while synchrotron and free-free emission dominate at approximately $40$ GHz and below.  

Extragalactic emission is mostly due to clusters of galaxies at large scales and the large background of resolved and unresolved radio and IR galaxies. Any large enough body generating hot ionised gas can produce Sunyaev-Zeldovich (SZ) effects. The SZ effect is due to the inverse Compton scatting of CMB photons on free electrons in an ionised medium. The thermal SZ effect results from photon scattering in high-temperature ionised electron gas, and the kinetic SZ effect results from Thompson scattering on electrons with a global radial bulk motion in respect to the CMB. Observations of these effects grant us an opportunity to study the physics of gas condensation in cluster-size potential wells at very large scales, allowing cosmological investigations of structure formation. Extragalactic "point sources" denote emissions which are not resolved by CMB surveys, from objects such as radio galaxies, IR galaxies, and quasars.

Finally, emissions from the solar system include "local" sources such as planets, satellites, asteroids, as well as the diffuse emission of the zodiacal light. There is also a matter of subtracting the contamination cause by the monopole and dipole contributions to the all-sky maps. \\

\begin{thebibliography}{99}

%%
%%  bibliographic items can be constructed using the LaTeX format in SPIRES:
%%    see    http://www.slac.stanford.edu/spires/hep/latex.html
%%  SPIRES will also supply the CITATION line information; please include it.
%%


\bibitem{ELWright}
E. L. Wright, paper presented at IAS CMB Data Analysis Workshop, 1996, astro-ph/9612006


\bibitem{TegmarkMap}
M. Tegmark, Astrophys. J. 480:L87-L90, 1997, astro-ph/9611130v2 

\bibitem{Paykari1}
P. Paykari, J. L. Starck, M. J. Fadili, 2012, arXiv:1202.4908

\bibitem{HoggForemanMackey}
S. Ambikasaran, D. Foreman-Mackey, L. Greengard, D. W. Hogg, M. O'Neil, 2014, arXiv:1403.6015v1


\bibitem{Rasmussen}
C. E. Rasmussen and C. Williams, "Gaussian Processes for Machine Learning", MIT Press, 2006. 

\bibitem{Schoelkopf}
B. Schoelkopf and A. J. Smola, "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", MIT Press, 2002. 


\bibitem{SpergelOh}
S. P. Oh, D. N. Spergel, and G. Hinshaw, ApJ, 510, 551, 1999, astro-ph/9805339v2 

\bibitem{Hinshaw}
G. Hinshaw, D. N. Spergel et al., ApJ. Suppl. 148:135, 2003, astro-ph/0302217

%\bibitem{Wandelt1}

%\bibitem{Wandelt2}


\bibitem{Natoli}
P. Natoli, G. de Gasperis, C. Gheller, N. Vittorio, Astron. Astrophys. 372:346, 2001, astro-ph/0101252v2 





%L. da Ponte, Trans. N. Y. Acad. Sci., {\bf 3}, 27 (1795).
%%CITATION = TNYAA,3,27;%%


\end{thebibliography}

\begin{itshape} 
EB would like to sincerely thank Professor David Hogg for providing the idea for this project, as well as an immeasurable degree of patience throughout the process so far.
\end{itshape}








\section*{Appendix 1: Map-making methodology}

We begin with raw data, which must be calibrated, cleaned of "glitches" data (e.g. cosmic ray hits), and checked for systematic errors (i.e. some non-random signal found in the time-ordered data TOD not caused by the sky). So, we are left with TOD, a list of the positions and temperatures of all the pixels observed, in chronological order, in each channel. (For WMAP, Planck, the TOD consists of pairs of pixel positions and the associated temperature difference.)

We then compress the TOD with some map-making algorithm to create sky maps. The motivation behind representing the TOD in maps is to compress the data into something computationally manageable without losing any information. WMAP maps have $\sim10^7$ pixels, Planck maps have around $\sim10^8$ pixels. Different bands of maps have different resolutions with beams not spatially invariant or isotropic.

Here assuming a symmetric beam profile, let's review the map-making algorithm used by WMAP (using notation from \cite{Natoli}):

We have an $n-$dimensional vector $\textbf{d}$ of the TOD $d_1,\dots,d_n$,  $N_d$ sky observations with a given scanning strategy and at some given sampling rate (e.g. three points per FWHM). We use this to estimate the map $\textbf{m}$, a vector $m_1,\dots,m_n$ representing $N_p$ temperature values, associated with sky pixels of dimension $\sim$FWHM$/3$ Our map is naturally pixelized and represented as a function of temperature, so $x_i$ denotes the temperature at pixel $i$. 

By linearity, we write the TOD vector $\textbf{y}$ as data $=$ signal $+$ noise, i.e.

\begin{equation}
\textbf{d}_t=\textbf{s}_t+\textbf{n}_t=\textbf{P}_{tp}\textbf{m}_p+\textbf{n}_t
\end{equation}

where $\textbf{s}$ is the source, temporally constant but spatially varying, $\textbf{P}$ is a $N_d\times N_p$ "pointing matrix" and $\textbf{n}$ is noise, temporal.

The pointing matrix $\textbf{P}_{tp}$ gives the weight of each pixel $p$ in time sample $t$. The nonzero row values correspond to pixels being observed, at the time denoted by the row; the nonzero column entries correspond to all the times a given pixel has been observed. The matrix itself is normally quite sparse. There are usually: one nonzero entry for a total power temperature observation, two nonzero entries for a differencing temperature observation, and three nonzeros for a total power polarization observation. 

Applying $\textbf{P}$ on a map "unrolls" the map on a TOD via a given scanning strategy, and applying $\textbf{P}^{T}$ on the TOD "sums" the values into a map. 

How do we estimate $\textbf{m}$? Algorithms are based on Generalized Least Squares, i.e. a multivariate Gaussian distribution is used to describe the statistical properties of detector noise (noise properties are also assumed to be piece-wise stationary), and therefore we assign a Gaussian noise likelihood function for the data time stream given the true map as

\begin{equation}
\mathcal{L}(d_t | m_p) = \frac{1}{|2\pi N|^{N_p/2}} \Bigg[-\frac{1}{2}(\textbf{d}^{T} - \textbf{m}^{T}\textbf{P}^{T})\textbf{N}^{-1}(\textbf{d} - \textbf{P}\textbf{m}) + \textnormal{Tr}(\textnormal{ln}(\textbf{N})\Bigg].
\end{equation}


That is, we minimize $\chi^2$ 

\begin{equation}
\chi^2 = \textbf{n}^{T}\textbf{A}\textbf{n} = (\textbf{d}^{T} - \textbf{m}^{T}\textbf{P}^{T})\textbf{A}(\textbf{d} - \textbf{P}\textbf{m})
\end{equation}

such that some nonsingular, symmetric matrix $\textbf{A}$ is the noise inverse covariance matrix, i.e. $\textbf{A}^{-1} = \textbf{N} \equiv\left<\textbf{n}\textbf{n}^T\right>$. If we take the noise to have a zero mean $\left<\textbf{n}\right>=0$, then the noise covariance matrix is $\textbf{N} \equiv\left<\textbf{n}\textbf{n}^T\right>$. 

We get an estimator $\widetilde{\textbf{m}}$ by deriving with respect to $\textbf{m}$

\begin{equation}
\widetilde{\textbf{m}} = (\textbf{P}^{T}\textbf{A}\textbf{P})^{-1}\textbf{P}^{T}\textbf{A}\textbf{d}.
\end{equation}

Provided $\left<\textbf{n}\right>=0$, then $\left<\widetilde{\textbf{m}}\right>=\textbf{m}$.

The map covariance matrix is then written as

\begin{equation}
\Sigma^{-1}= \langle(\textbf{m}-\widetilde{\textbf{m}})(\textbf{m}^{T}-\widetilde{\textbf{m}}^{T})\rangle=(\textbf{P}^{T}\textbf{AP})^{-1}\textbf{P}^{T}\textbf{A}\langle\textbf{nn}^{T}\rangle\textbf{AP}(\textbf{P}^{T}\textbf{AP})^{-1}
\end{equation}

We find $\textbf{A}$ that minimizes the variance of $\widetilde{\textbf{m}}$ in order to have a so-called loise noise estimator, which is exactly $\textbf{A}^{-1}=\textbf{N}\equiv\langle\textbf{nn}^{T}\rangle$. In this case,  $\widetilde{\textbf{m}}$ of the minimum variance (linear and unbiased) estimator. 

We then write the GLS solution as 

\begin{equation}
 \widetilde{\textbf{m}}=\Sigma^{-1}\textbf{P}^{-1}\textbf{N}^{-1}\textbf{d}
\end{equation}

such that 

\begin{equation}
\Sigma=\textbf{P}^{T}\textbf{N}^{-1}\textbf{P}
\end{equation}

If the detector noise distribution is in fact Gaussian, the $\widetilde{\textbf{m}}$ is indeed the maximum likelihood estimator. 

We write the map-making equations as 

\begin{equation}
N^{-1}_{pp'}=P^{T}_{tp}N^{-1}_{tt'}P_{t'p'}
\end{equation}
\begin{equation}
z_p=P^{T}_{tp}N^{-1}_{tt'}d_{t'}
\end{equation}
\begin{equation}
d_p=N_{pp'}z_{p'}
\end{equation}

resulting a number of sky maps at different frequencies. Sky maps from different channels at the same frequency are written such that each frequency map is represented as the weighted average of all the maps of the different channels at that frequency. For purely cosmological purposes, such maps retain all cosmological information from the TOD. One can therefore measure parameters just as accurately from the maps as from the TOD. (Please refer to \cite{ELWright} and \cite{TegmarkMap} for more background.)

\section*{Appendix 2: Spherical Harmonics Convention}

Recall Laplace's equation, a second-order partial differential equation written as

\begin{equation}
\nabla^2\phi=0
\end{equation}

where $\phi$ denotes a scalar function. In spherical coordinates, we write this expression as 

\begin{equation}
\nabla^2 f=\frac{1}{\rho^2}\frac{\partial}{\partial\rho}\Bigg(\rho^2\frac{\partial f}{\partial\rho}\Bigg)+\frac{1}{\rho^2\sin\theta}\frac{\partial}{\partial\theta}\Bigg(\sin\theta\frac{\partial f}{\partial\theta}\Bigg)+\frac{1}{\rho^2\sin^2\theta}\frac{\partial^2 f}{\partial\phi^2}=0
\end{equation}

Spherical harmonics are simply the angular set of solutions to this equation such that the set $Y_{lm}$ forms an orthogonal basis. We define $Y_{lm}$ as

\begin{equation}
Y_{lm}(\theta, \phi)=\lambda_{lm}(\cos\theta)e^{im\phi}
\end{equation}

\begin{eqnarray*}
\lambda_{lm}(x)=\sqrt{\frac{2l+1}{4\pi}\frac{(l-m)!}{(l+m)!}}P_{lm}(x), \ \ \mbox{ for }m\geq0 \\
\lambda_{lm}=(-1)^{m}\lambda_{l|m|} \ \mbox{ for }m<0 \\
\lambda_{lm}=0, \mbox{ for } |m|>l
\end{eqnarray*}


Using $x\equiv\cos\theta$, the associated Legendre Polynomials $P_{lm}$ solve the differential equation

\begin{equation}
(1-x^2)\frac{d^2}{dx^2}P_{lm}-2x\frac{d}{dx}P_{lm}+\Bigg(l(l+1)-\frac{m^2}{1-x^2}\Bigg)P_{lm}=0
\end{equation}

Note that $P_{lm}$ is related to the ordinary Legendre Polynomials $P_l$ such that 

\begin{equation}
P_{lm}=(-1)^m(1-x^2)^{m/2}\frac{d^m}{dx^m}P_l(x)
\end{equation}

such that $P_l(x)$ are given by the Rodrigues formula

\begin{equation}
P_l(x)=\frac{1}{2^l l!}\frac{d^l}{dx^l}(x^2-1)^l
\end{equation}



 
\end{document}
