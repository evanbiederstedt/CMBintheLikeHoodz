\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage{amsfonts} %allows use of mathbb
\usepackage{gensymb} %allows use of degree symbol, otherwise use ^{\circ}
\usepackage{amsmath} %allows use of \text{} command

\begin{document}
%Header-Make sure you update this information!!!!
%\noindent
%\large\textbf{Likelihood} \hfill \textbf{Evan Biederstedt} \\
%\normalsize Discuss First paper, likelihood, etc. 
%\hfill \ 12/05/2014 \\



\section*{Misc Notes to self, mid March 2015}

\textbf{http://arxiv.org/pdf/1409.7718.pdf}

The covariance matrix \textbf{C} of temperature maps $T^i$ is written as

$$
C_{ij}=<\Delta T_i\Delta T_j>=\frac{1}{4\pi}\sum^{N_\text{pix}}{p=1}( T^{i}(p)-\bar{T}^{i})( T^{j}(p)-\bar{T}^{j})
$$

\textbf{COBE info}

From Seljak, Zaldarriaga:

There have been several studies intended to constrain cosmological parameters with the existing CMB
data Lineweaver 1998, Tegmark 1999, Dodelson, Knox 1999. 


http://arxiv.org/pdf/astro-ph/0603369.pdf

http://arxiv.org/pdf/astro-ph/9809001.pdf

\textbf{Question to self}

Planck confirms full-sky maps are *not* isotropic. WMAP found same thing. 

Our covariance relation follows from isotropy. Question: how does that affect this? 




\textbf{Fantastic paper on low l likelihood calculation}
http://arxiv.org/pdf/astro-ph/0606088v2.pdf
A RE-ANALYSIS OF THE THREE-YEAR WILKINSON MICROWAVE ANISOTROPY PROBE
TEMPERATURE POWER SPECTRUM AND LIKELIHOOD

A. LOW-ℓ LIKELIHOOD EVALUATION
Low-ℓ likelihood evaluation from high-resolution data has received considerable attention in the last few years
(Efstathiou 2004; Slosar et al. 2004; Hinshaw et al. 2006). The reason is simply that while the currently popular
pseudo-spectrum power spectrum estimators (Hivon et al. 2002) work very well on intermediate and small angular
scales, they are clearly sub-optimal on large scales.



\textbf{From WMAP explanatory supplement}

p. 71, SKY MAPS

Iterative algorithms are used to create skymaps from the calibrated differential time-ordered WMAP data for each of the ten differencing assemblies. Each pixel in a map represents a skyp temperature. The CMP dipole has been removed from the Stokes I maps. 

Frequency band maps were created by computing a weighted, pixel-by-pixel, mean of the individual differencing assmeblines comprising each bandpass. 


\textbf{Note on Legendre Polynomials and other special functions}

The Legendre polynomial $\mathcal{P}_l(\mu)$ is an $l$th-order polynomial of $\mu$. For $-1\geq\mu\geq+1$, $\mathcal{P}_l$ has zeroes in this interval. They are othogonal so that 

$$
\int^{1}_{-1}d\mu\mathcal{P}_l(\mu)\mathcal{P}_{l'}(\mu)=\delta_{ll'}\frac{2}{2l+1}
$$

The recurrence relation is

$$
(l+1)\mathcal{P}_{l+1}(\mu) = (2l+1)\mu\mathcal{P}_l(\mu) - l\mathcal{P}_{l-1}(\mu)
$$

For values of $l$, 

$$
\mathcal{P}_0(\mu)=1; \ \ \ \mathcal{P}_1(\mu) = \mu; \ \ \ \mathcal{P}_2(\mu) = \frac{3\mu^2-1}{2}
$$

We can express the Legendre polynomial in terms of a sum of products in the spherical harmonics:

$$
\mathcal{P}_l(\hat{x}\cdot\hat{x}')=\frac{4\pi}{2l+1}\sum^{l}_{m=-l}Y_{lm}(\hat{x})Y^{*}_{lm}(\hat{x}')
$$

where spherical harmonics are defined as usual, i.e. eigenfunctions of the angular part of the Laplacian, which hold orthogonal relation. 


Relation between Legendre polynomials and spherical Bessel functions:

$$
\frac{1}{2}\int^{1}_{-1}d\mu\mathcal{P}_l(\mu) e^{iz\mu} = \frac{j_l(z)}{(-i)^l}
$$

and


$$
e^{i\vec{k}\cdot\vec{x}} = \sum^{\inf}_{l=0} i^{l} (2l+1) j_{l}(kx)\mathcal{P}_l(\hat{k}\cdot\hat{x})
$$

\textbf{Notes about hwo to calculate Legendre polynomials}

%http://people.sc.fsu.edu/~jburkardt/cpp_src/legendre_polynomial/legendre_polynomial.html

The Legendre polynomial $P(n,x)$ can be defined by: 

$$
P(0,x) = 1
$$

$$
P(1,x) = x
$$

$$
P(n,x) = \frac{(2n-1)}{n} x P(n-1, x) - \frac{(n-1)}{n} P(n-2, x)
$$

where $n$ is a non-negative integer. 

The N zeroes of $P(n,x)$ are the abscissas used for Gauss-Legendre quadrature of the integral of a function $F(X)$ with weight function 1 over the interval $[-1,1]$.


%From FORTRAN 77 book, Chapter 4

The idea of Gaussian quadratures is to give ourselves the freedom to choose not only the weighting coefficients, but also the location of the abscissas at which the function is to be evaluated. 

Given $W(x)$ and a given integer $N$, we can find a set of weights $w_j$ and abscissas $x_j$ such that the approximation 

$$
\int^b_a W(x) f(x) dx \approx \sum^N_{j=1}w_j f(x_j)
$$

is exact if $f(x)$ is a polynomial. 

For Gauss-Legendre polynomials, the weight functions, intervals, and recurrence relations that generate these orthogonal and Gaussian quadrature formulas: 

$$
W(x)=1, \ \ \ -1<x<1
$$

$$
(j+1) P_{j+1} = (2j+1) x P_j - jP_{j-1}
$$

We can rewrite this recurrence relation as 

$$
(n+1)P_{n+1}(x) = (2n+1) x P_{n}(x) - n P_{n-1}(x)
$$

RELATION BETWEEN SPHERICAL HARMONICS AND LEGENDRE POLYNOMIALS

The spherical harmonic $Y_{lm}(\theta, \phi)$, $-l\leq m \leq l$ is a function of the two coordinates $\theta, \phi$ on the surface of a sphere. 

The spherical harmonics are orthogonal for different $l$ and $m$, and they are normalized so that their integrated square over the sphere is unity: 

$$
\int^{2\pi}_{0} d\phi \int^{1}_{-1} d(\cos\theta) Y_{l'm'}^{*}(\theta, \phi) Y_{lm}(\theta, \phi) = \delta_{l'l}\delta_{m'm}
$$

where the asterisk $*$ denotes complex integration. 

Mathematically, the spherical harmonics are related to associated Legendre polynomials by the equation

$$
Y_{lm}(\theta, \phi) = \sqrt{\frac{2l+1}{4\pi}\frac{(l-m)!}{(l+m)!}}P^m_l(\cos\theta) e^{i m \phi}
$$

By using the relation, 

$$
Y_{l,-m}(\theta, \phi) = (-1)^m Y_{lm}^{*}(\theta, \phi)
$$

we can always relate a spherical harmonic to an associated Legendre polynomial with $m\geq0$. With $x\equiv\cos\theta$, these are defined in terms of the ordinary Legendre polynomials 

$$
P^m_l(x)=(-1)^m(1-x^2)^{m/2}\frac{d^m}{d x^m}P_l(x)
$$

The following recurrence on $l$ is stable:

$$
(l-m)P^m_l = x(2l-1)P^m_{l-1}-(l+m-1)P^m_{l-2}
$$

%%See page 247 for a full FORTRAN 77 code of this function. 


MISC. NOTES %FROM DODELSON

We define the $l$th multipole moment of the temperature field as 

$$
\Theta_l\equiv \frac{1}{(-i)^l}\int^{1}_{-2}\frac{d\mu}{2}\mathcal{P}_l(\mu)\Theta(\mu)
$$

where $\mathcal{P}_l$ is the Legendre polynomial of order $l$. The photon perturbation variable is $\Theta(k,\mu,\eta)$, the Fourier transform of $\delta T/T$, the fractional temperature difference. Note we use the conformal time $\eta$ as the evolution variable. The photon distribution depends not only on $\vec{x}$ and time, but also on the direction of propagation of the photon, $\hat{p}$. Therefore, in Fourier space, the photon perturbations depend not only on $k$ and $\eta$, but also on $\vec{p}\cdot\vec{k}$, which we defined as $\mu$. That is, the variable $\mu$ describing the direction of photon propagation is defined as the cosine of the angle between the wavenumber $vec{k}$ and the photon direction $\hat{p}$ to be 

$$
\mu\equiv \frac{\vec{k}\cdot\hat{p}}{k}
$$

The wavevector $\vec{k}$ is pointing in the direction in which the temperature is changing, so it is perpendicular to the gradient. 

CONCEPTUAL CRUTCH FOR LEGENDRE POLYNOMIALS: the higher order ones vary on smaller scales than do the low-order ones. In general, $\mathcal{P}_l$ crosses zero $l$ times between $-1$ and $+1$. 

\textbf{BEST INTRODUCTION TO OUR PROJECT I'VE READ: astro-ph/0303414.pdf}

The accurate analysis of CMB data places strong de- mands on the statistical methods employed. Even for to- tal intensity data, which is simpler than polarization, the extraction of the power spectrum from upcoming mega- pixel datasets with standard maximum-likelihood methods (e.g. Bond, Jaffe, Knox 1998) is beyond the range of any supercomputer. [The operations count scales as the number ofpixelscubed,Np3ix,whilethestoragerequirementsare O(Np2ix).] In the search for fast alternatives to brute-force maximum-likelihood power spectrum estimation, two broad approaches have emerged. In the first, experiment-specific symmetries are exploited to make the brute-force analysis tractable, or, if the symmetries are only approximate, to pre-condition an iterative solution to the likelihood max- imisation. An example of the former is the ingenious “ring- torus” method of Wandelt ,Hansen (2003), while the lat- ter was pioneered by Oh, Spergel ,Hinshaw (1999) during the development of the pipeline for the WMAP satellite. The second class of methods sacrifice optimality in favour of speed by adopting a more heuristic weighting of the data (such as inverse weighting with the noise variance). An es- timate of the underlying power spectrum is then obtained from the raw, rotationally-invariant power spectrum of the weighted map (the pseudo-Cls; Wandelt, Hivon , G ́orski 2001) either by a direct linear inversion (Szapudi, Prunet ,Colombi 2001; Hivon et al. 2002) or with likelihood methods (Wandelt et al. 2001; Hansen, Ǵorski , Hivon 2002). The linear inversion, which yields estimators quadratic in the data, can be performed directly in harmonic space (Hivon et al. 2002), or, more simply, by first transforming to real space (i.e. by constructing the correlation function) and then recovering the power spectrum with a (suitably apodized) integral transform (Szapudi et al. 2001). The estimation of polarization power spectra is less well explored than for total intensity, although all of the above methods can, in principle, be extended to handle polarization. To date, only brute-force maximum likelihood (Kovac et al. 2002, Munshi et al. in preparation), minimum-variance quadratic estima- tors (Tegmark , de Oliveira-Costa 2002), pseudo-Cl meth- ods with statistical (Hansen , G ́orski 2003) or direct inver- sion (Kogut et al. 2003) in harmonic space, and real-space correlation function methods (Sbarra et al. 2003) have been demonstrated on polarized data. Of these, only the pseudo- Cl methods are fast enough to apply many times (e.g. in Monte-Carlo simulations) to mega-pixel maps. In this paper we extend the fast correlation-function approach of Szapudi et al. (2001) to polarized data.



\textbf{SEE DODELSON, CHAPTER 11.3 ESTIMATING THE LIKELIHOOD FUNCTION}



To illustrate the need for new techniques of likelihood computation, let us consider a concrete example: the data set from the Boomerang anisotropy experiment (Net- terfield et ai, 2001). There are 57,000 pixels on the sky covered in this set. The the- ory and noise covariance matrices are both nondiagonal and both 57,000 x 57,000 dimensional matrices. Inverting these beasts with present computers is possible, although slow." If we needed to invert only once, this might be acceptable. But, we need to evaluate the likelihood function at many points in parameter space to find its maximum and the region at which it falls to, say, 5% of its maximum. This would be barely manageable if the parameter space was one-dimensional. A one- dimensional fit, though, would lose most of the information contained in the map. The data are actually sensitive to the power on many different scales. Therefore, the parameter space — the amplitude of the power on these many different scales — is multidimensional, ''multi-'' here of order 20. The likelihood function should in principle be computed about 10 times in each dimension, for a total of 10 com- putations. Since each inversion takes several hours, this is not feasible. All of these estimates are for the Boomerang experiment. The MAP satellite will have 10 times as many pixels and be sensitive to a wider range of scales. Planck will be more sen- sitive still. Thus, we need new techniques, shortcuts, for evaluating the hkelihood function and finding its maximum and its width.

TYPE UP KARHUNEN-LOEVE TECHNIQUES AND OPTICAL QUADRATIC ESTIMATOR DERIVATIONS, P. 




SUBSEQUENT QUESTION: COULD WE WORK ON POLARIZED DATA? IT'S WORTH A THOUGHT. " However, with incomplete sky coverage, separating the polarization field into electric and magnetic components is no longer straightforward. Exquisite monitoring of leakage between E and B in analysis pipelines will be required if primordial B polarization is to be de- tected down to the fundamental confusion limit set by cos- mic shear (Kesden, Cooray , Kamionkowski 2002)."


Papers on constructing covariance $C_l$ matrix: \\
http://arxiv.org/pdf/astro-ph/0410097.pdf \\
http://arxiv.org/pdf/astro-ph/9808264.pdf \\
http://arxiv.org/abs/astro-ph/0307515 \\
http://arxiv.org/pdf/astro-ph/0608662.pdf \\
http://arxiv.org/pdf/astro-ph/9803272.pdf Whoah, David Charbonneau's first arXiv paper is on CMB with Dick Bond. \\
http://arxiv.org/pdf/astro-ph/0410097.pdf Error properties of estimators, Challinor


Where $l$ is the angular scale, we compute $\sum_m a_{lm}a^{\dagger}_{l'm'}=(2l+1)C_l$ gives the power at a given angular scale. 

\textbf{Reminder of effects of the sky cut, in particular mode mixing}


Pseudo power spectra and the effect of the mask/sky cut. Incomplete sky coverage affects our estimation of the angular power spectrum of a given map. In the incomplete sky, the spherical harmonics are no longer a orthonormal base function, and the inversion of the temperature map into alm produces an artificial coupling of modes. The power spectrum can be corrected by approximate factor $1/f_{\text{sky}}$ where $f_{\text{sky}}$ is the fraction of the total sky observed (this works only for not-so-restrictive masks.) 
The effect of the mask can be written in terms of a mixing matrix, i.e.

$$
<\tilde{C}_l>=\sum_{l'}M_{ll'}C_{l'}
$$

And there's also the MASTER code (Hivon et al, 2002), which provides analytical expressions for the mixing matrices. 

$$
M_{l_1 l_2}=\frac{2l_2+1}{4\pi}\sum_{l_3}(2l_3+1)W_{l_3}\Big(\begin{matrix}
l_1&l_2&l_3\\ 0&0&0
\end{matrix}\Big)^2
$$

%http://workshops.ift.uam-csic.es/uploads/charla/150/Healpix.pdf

Current details of likelicood code used for low and high ells, as well as masks used, found here:

******

To build HFI maps, we use the destriping approximation, in which noise is assumed to decompose into two components : white noise plus low frequency drifts. Using the sky redundancy, the low frequency drifts are modelled as one constant, or offset, per pointing period. To speed up the ulterior processing we first build intermediate products, by taking advantage of redundancies : we average signal and detector orientation on healpix pixels visited during each fixed pointing period, which we call hereafter 'ring'. Detector's pointing are corrected for slow drifts and aberration (displacement on the sky indouced by the satellite's motion).

Expected systematics: \begin{itemize}

\item cosmic rays

\item"elephants" (cosmic rays at 100 $mK$) which causes the temperature to vary, inducing small temperature fluctuations, and noise variations in the detectors

\item thermal fluctuations---although HFI stable, still small thermal fluctuations

\item popcorn noise---"split level" noise

\item jumps

\item 4K Cooler noise----noise with very specific frequency, 4K cooler line stability

\item etc.
 
\end{itemize}

HPR: HEALPix Rings are introduced to avoid any additional binning of the data. We choose a
sky pixelization as a basis for this ring making. HPR are therefore partial sky maps produced via a projection onto the sky of each single pointing period separately.


\textbf{SO STRANGE}
The inpainting consists in replacing some pixels (as indicated by the mask named $INP\_MASK$) by the values of a constrained Gaussian realization which is computed to ensure good statistical properties of the whole map (technically, the inpainted pixels are a sample realisation drawn under the posterior distribution given the unmasked pixels. 


FEBeCoP software enables fast, full-sky convolutions of the sky signals with the Effective beams in pixel domain. Hence, a large number of Monte Carlo simulations of the sky signal maps map convolved with realistically rendered, spatially varying, asymmetric Planck beams can be easily generated. We performed the following steps:
• generate the effective beams with FEBeCoP for all frequencies for dDX9 data and Nominal Mission
• generate 100 realizations of maps from a fiducial CMB power spectrum
• convolve each one of these maps with the effective beams using FEBeCoP
• estimate the average of the Power Spectrum of each convolved realization, %$C'_\ell_'^out^'$, and 1 sigma errors
As FEBeCoP enables fast convolutions of the input signal sky with the effective beam, thousands of simulations are generated. These Monte Carlo simulations of the signal (might it be CMB or a foreground (e.g. dust)) sky along with LevelS+Madam noise simulations were used widely for the analysis of Planck data. 

Begin with 531 billion raw data samples from HFI. 


\textbf{Whimsical thoughts on what we're doing: inference applied to CMB experiments}

Let's consider what we're doing in very general terms, and summarize CMB data and probablisitic inference in a nutshell:

We begin with maps, which are a combination of pure CMB signal, noise, and foregrounds (including MW galaxy, extragalactic point sources, zodiacal light, dipole modulation effect). These foregrounds are diverse and interesting in their own right. We could develop/use new inference methods to extract these foregrounds---this is a future goal. However, let's just concentrate on the pure CMB signal for a moment.  

Foreground seperation: any "errors" or effects from how methods of foreground separation/removal extract the pure CMB signal propagate into cosmological analysis. Even with the very best foreground methods (and Planck used four methods to cross-correlate/check results), we're left with regions we must mask/sky cuts, i.e. we must mask the Galactic plane, point sources, bad pixels, etc. 

From these cleaned CMB maps with masked portions of the sky (i.e. lost/covered information), masked skies are used. Now, in order to extract statistical nature of cosmological information, one computes the angular power spectrum. With regards to Planck: At small angles (high ells), a frequentist estimator is used. At low ells, mostly due to the masked Galactic plane, Gibbs sampling is used (which is why greater uncertainities still remain at $l<100$ in comparison to higher ells).

Now, any deviations from the "expected" statistical $\Lambda$CDM with respect to what we find in our pure CMB signal is *by definition* considered part of the "CMB Anomalies". These anomalies can be thought of as coming from four categories: obvious problems with the data analysis (e.g. how one extracts foregrounds), obvious anisotropies (North-South asymmetry in power spectrum, octupole/quadrupole/low-order ell aligment from Tegmark, Magueijo's Axis of Evil 2006, etc.), localized features (the "Cold Spot", a region of temperature decrement, from Vielva 2004, Cruz 2005), or something else entirely (even/odd magnetic multipoles, magnetic fields, etc.). Any detection of primordial non-Gaussianity is also a statistical anomaly in the CMB signal. What's the relevance of this? To be short and concise: the use of $C_l$ estimators on cleaned CMB are also used in tests of anisotropies and other anomalies. So, evaluation of non-approximate $C_l$ affects these studies. 

Here's where we come in:

The CMB full-sky surveys uses largely two methods to estimate the angular power spectrum in the presence of sky cuts and noise. The first class of methods calculates a "pseudo power spectrum" (a weighted spherical harmonic transform of a map) and other estimators which have been developed/refined (e.g. quadratic estimator/QML estimator) on a sky cut. The second class of methods takes a likelihood approach. 


If the CMB temperature fluctuation $\Delta T$ is Gaussian distributed, then each $a_{lm}$ is an independent Gaussian deviate with 

$$
<a_{lm}>=0, \ \ \ <a_{lm}a^{*}_{l'm'}>=\delta_{ll'}\delta_{mm'}C_l
$$

where $C_l$ is the ensemble average power spectrum predicted by models, and $\delta$ is the Kronecker symbol. The actual power spectrum realized in our sky is

$$
C^{\text{sky}}_l=\frac{1}{2l+1}\sum^{l}_{m=-l}\vert a_{lm} \vert^2
$$

In the absence of noise and with FULL sky coverage, this is an unbiased estimate of the underlying theoretical power spectrum, limited only by cosmic variance. 


Recall: since temperature measured on the sky is modified by properties of the instrument (noise, beam), we use the assumption that the "observed power spectrum" estimates the true underlying power spectrum. 

For auto-/cross-correlation between maps $i$ and $i'$, we find a spectrum 

$$
C^{ii'}_l=w^{ii'}_lC^{\text{sky}}_l+N^{ii'}_l
$$

where $w^{ii'}_l=b^{i}_l{}b^{i'}_{l}p^{2}_{l}$ is the window function that describes the combined smoothing effects of the beam and finite sky map pixel size. And on average, the observed spectrum estmates the underlying power spectrum $C_l$, is 

$$
<C^{ii'}_l>=w^{ii'}_lC^{\text{sky}}_l+<N^{ii'}_l>\delta_{ii'}
$$

where $<N^{ii'}_l>$ is the average noise power spectrum for differencing assembly $i$ and the delta function means noise is uncorrelated between differencing assemblies. So, to estimate the underling spectrum on the sky $C_l$, the effects of noise bias and beam convolution must be removed. 

The weighting scheme is chosen so as to mimic the maximum likelihood estimation. Ok then... That's ALL a wee bit subjective, and NOT probablistic inference. Bad WMAP team, bad!

It's a problem for *all* $C_l$ estimators: how do we chose the weight function? 

See K. Smith, 2006

"An important practical issue, for both pure and ordinary pseudo-Cl estimators, is choosing the pixel weight function W(x). In general, this must be done empirically (perhaps guided by heuristics) using Monte Carlo simulations to optimize the estimator variance. The optimal weight function will depend on l; at low l or high signal-to-noise, uniform weighting (which minimizes sample variance) is near-optimal, whereas at high l or low signal-to-noise, inverse noise weighting (which minimizes noise variance) is near-optimal. For pure pseudo-Cl estimators, an extra complication arises when choosing the weight function. The statistical weight of a pixel is given by a combination of the weight function W(x) and its first two derivatives in the pixel.  "

As for the second class of methods taking the likelihood approach: the idea behind much thinking at this time is to evaluate the maximum likelihood. For instance, let's introduce a noise model $\textbf{d}=A\textbf{T}+n$ where $A$ is the pointing matrix. You could then write down a likelihood $P$(theory $\vert$ data) is 

$$
\frac{1}{\vert(2\pi)^{N/2}\textbf{C}\vert^2}\exp[-\frac{1}{2}(d-AT)^{T}\textbf{C}^{-1}(d-AT)]]
$$

and then find the solution $\hat{T}$ which maximizes the likelihood function, giving GLS. 

$$
\hat{T}=(A^{T}\textbf{C}^{-1}A)^{-1}(A^{T}\textbf{C}^{-1}d)
$$

For our purpose: \textbf{discuss covariance matrix in spherical harmonic space versus pixel space.} 
The likelihood estimation is as follows, using covariance is $\textbf{C}=\textbf{S}+\textbf{N}$:

$$
\mathcal{L}(C_l\vert \textbf{d})=P(\textbf{d}\vert C_l) = \frac{\exp(-\frac{1}{2}\textbf{d}^{T}\textbf{C}^{-1}\textbf{d})}{\sqrt{\text{det}\textbf{C}}}
$$

In the spherical harmonic basis, we have: 
\begin{itemize}
\item[]\textbf{d} is $a_{lm}$
\item[]\textbf{S} is $\text{diag}(C_2, C_2,\dots C_3, C_3, \dots)$
\item[]\textbf{N} is $N_{(lm)(lm)'}$
\end{itemize}

In the pixel basis, we have: 

\begin{itemize}
\item[]\textbf{d} is $T_i$
\item[]\textbf{S} is $\sum_{l}\frac{(2l+1)}{4\pi}C_l P_l (\cos\theta_{ij})$
\item[]\textbf{N} is $\sigma^2_i\delta_{ij}$
\end{itemize}

In the pixel basis, the noise covariance is *nearly* entirely diagonal. In the spherical harmonic basis, it is rather the signal covaraince which is nearly entirely diagonal. 
t

Thus, if we take two full skies with artificially created data that are *exactly* Gaussian, then the likelihood for pixel space and real space should be the same. This is the first order of business. 



\paragraph{} \hspace{0pt} \\


\textbf{Typical views of the CMB community regarding calculating power spectrum likelihood}

The CMB community views are "Trying to directly compute the power spectrum likelihood impossible! Therefore, develop approaches to power spectrum estimation."

Planck, Paper XV, 2013 arXiv:1303.5075

"The main problem with the likelihood expression given in Eq. 21 (i.e. the CMB power spectrum likelihood function) is its high computational cost. This is determined by the matrix inversion and determinant evaluations, both of which scale as $O(N^3)$ with $N = n_T + 2n_P$. In practice, this approach is therefore limited to coarse pixelizations, $Nside ≤ 16$, which reliably only supports multipoles below $l 􏰅<30$. On the other hand, the Gaussian approximation adopted by the high-l likelihood is not sufficiently accurate for the stringent requirements of Planck below l 􏰅<50. In the next section, we therefore describe a faster low-l likelihood estimator, based on Gibbs/MCMC sampling, which allows us to exploit the full range up to $l ≤ 50$ with low computational cost, while additionally supporting physically motivated foreground marginalization."

On the other hand, the idea that a "non-approximate likelihood is not tractable/impossible" has become common knowledge in this community for close to 20 years now. See any and all the WMAP/Planck papers, and subsequent analyses. 

Knox, Jaffe, Bond papers
http://arxiv.org/abs/astro-ph/9903166
http://arxiv.org/abs/astro-ph/9708203

Showing this is possible would be a huge step forward. 

If assume ideal, noise measurements of temperature over the FULL SKY, we use estimators. Problem: we have instrumental noise, noise which is heteroskedastic, i.e. not stationary/ uneven over different sky regions due to how measurements made/how many passes detector makes (i.e. different regions of the sky observed different times), and partial sky coverage/masked sky due to Galactic plane. This makes estimating the power spectrum complicated. Modleling beam effects is tricky as well, because maps at different channels have different resolutions with beams, beams not necessarily isotropic or spatially invariant. 

Further problems with power spectrum estimation: one never has fully sky coverage, and one should (SOMEHOW! SUBJECTIVELY?) give less weight to noisy pixels so as not to destroy information. That destroys orthogonality of spherical harmonics. 

\paragraph{} \hspace{0pt} \\

\textbf{Why are Sky Cuts Bad/What do they do?}
Well, incomplete sky coverage increases sample variance by approx. $1/\sqrt{\text{Covered \ Area}}$ and smears out features in the power spectrum. 
With sky cuts, part of the signal is discarded along with contamination.

Note that at low ell, $l>50$ or so, uncertainities arise due to masked areas, primarily the Galactic plane. (There are also effects of "bad pixels" due to point sources, etc., but this mostly affects larger ells. However, Planck takes effects of point sources on low ells very seriously, see paper XV, 2013.)  There is no objective guide to how "clean" these pixels are though, beyond foreground removal techniques judged by chi-squared metric.)

See George Efstathiou for review of estimators. astro-ph/0307515
"Myths and Truths Concerning Estimation of Power Spectra", 2004
Discusses methods for power spectrum estimators and for deriving an accurate covariance matrix. States orthodox CMB community view that directly computing the power spectrum likelihood is computationally prohibitive/impossible. You either use strong assumptions or calculate approximate power spectrum. 

Typical statement: "In order to use the power spectrum for cosmological parameter estimation, you need to know the complete likehood function $P(d | C_l(\theta))$ where $\theta$ denotes cosmological parameters. So, power spectrum compution should give model spectra likehood function. PROBLEM: this evaluation is not computationally feasible at full-map resolution and hence there are different methods to calculate the likelihood function at low ell and high ell. Usually, the low ell algorithms use low resolution maps and do Bayesian inference. The high ell estimators use  frequentist estimators. 

Another typical statement, Peiris thesis, WMAP paper Likelihood function, p.20:

""
Because of the foreground sky-cut, different multipoles are correlated and only a fraction of the sky, $f_{\text{sky}}$, is used in the analysis. In this case, it becomes computationally prohibitive to compute the exact form of the likelihood function. So, there are several different approximations used in the CMB literature for the likelihood function. 

The power spectrum covariance encodes the uncertainities in the power spectrum due to cosmic variance, detector noise, point sources, the sky cut, and systematic errors. 

p.13

The sky cut has two significant effects on the power spectrum covariance matrix. 


""


As an example, some low-ell estimators use Gibbs sampling, i.e. samples from possible realization of the power spectrum likelihood given experimental data and then builds complete statistical description of each individual multipole component, $C_l$, of the power spectrum. 

For reference, let's introduce standard pseudo-power spectrum scheme MASTER from Eric Hivon, Krys. Gorski, Netterfield, et al, 2002, (all currently Planck people). "MASTER of the CMB anisotropy power spectrum: a fast method for statistical analysis of large and complex CMB data sets." arXiv:astro-ph/0105302
The MASTER method is a binned pseudo-$C_l$ estimator, i.e. pseudo meaning calculated on map with missing/removing pixels and no corrections have been made for coupling between harmonic modes which are introduced in the sky cut. 

Also take a look at Ben Wandelt, Hivon, Gorski, "Pseudo Cl method", 2001, arXiv:astro-ph/0008111

NOTE: this method gives ell mode-mode coupling, which is induced by incomplete sky coverage/sky cuts/masked sky. WE WANT TO FIX THIS. 


\paragraph{} \hspace{0pt} \\

\textbf{Future ideas for foreground seperation techniques}

NOTE TO SELF, APRIL 2: actually, it would be easier to begin with ILC type methods in real space with entire convariance matrix. See Delabrouille paper. 

""One uses the fact that the expected variance of a sum of two components is larger than the variance of either of the components. The CMB map is modeled as a linear combination of data vectors at different WMAP frequencies with the constraint on the coefficients ensuring that the CMB signal is preserved in the linear combination. The CMB map is determined by minimizing the variance across the sky of the linear combination map.""

We begin with 

$$
a_{lm}=\frac{C^{\text{CMB}_l}}{C^{\text{clean}_l}}a_{lm}
$$

but in real space. Minimize variance. It should work like a dream.

I'M REALLY INTERESTED IN TRYING THIS IF WE CAN CALCULATE A NON-APPROXIMATE CMB LIKELIHOOD FUNCTION. 

SEE http://arxiv.org/pdf/1202.1034.pdf

******

For non-approximate likelihood, if we have $C_l$, we have improved the ICA methods (recall: ICA optimizes non-Gaussian features of latent variables to pull out). Furthermore, if we can solve the entire spectral covariance matrix, we can give non-approximate SMICA results. Talk to Hogg about this. 

Summary: SMICA is the ICA-based "main product" for cleaned CMB map. It combvines channels with $l-$dependent weights, with optimal weights determined from a Maximum Likelihood fit of a "semi-parametric" model. Again, remaining Galactic mask still must be inpainted for final product release. 

For signal plus noise, SMICA adjusts weight to multipole ell, allowing weights to depend on angular frequency. 

Where do those weights come from? (recall graphs from baccigalupi) $d=As+n$, with the mixing matrix equal to A, the parameters $\theta=A$, and the $\text{dim}(\theta)$ equal to $N_{\text{chan}}\times N_{\text{comp}}$. 

SMICA models each Planck channel as a noisy linear mixture of CMB and 6 foregrounds, using decorrelation between foregrounds and CMB. Completely blind method. All non-zero parameters free. 

First, combine channels in harmonic space: 

$$
s^{\dagger}_{lm}=\textbf{w}^{\dagger}_{l}\textbf{d}_{lm}
$$

Assume coherent CMB:

$$
\textbf{d}_{lm}=\textbf{a} s_{lm} +\text{contanimation}_{lm}
$$

Best weights for known $C_l=\text{Cov}(\textbf{d}_{lm})$: HERE IS WHERE WE MAKE THE IMPROVEMENT

$$
\textbf{w}_l=\frac{\textbf{C}^{-1}_{l}\textbf{a}}{\textbf{a}^{\dagger}\textbf{C}^{-1}_{l}\textbf{a}}
$$

But spectral matrix $\textbf{C}_l$ is unknown, so:

At high ell, fear not and take

$$
\hat{\textbf{C}}_l=\frac{1}{2l+1}\sum_{m}\textbf{d}_{lm}\textbf{d}^{\dagger}_{lm}
$$

At low ell, model $C_l(\theta)$ and fit

$$
C_l(\hat{\theta})=\text{max}_{\theta} \text{P}(\hat{\textbf{C}}_l \vert C_l(\theta))
$$

GOALS: Improve SMICA with full knowledge of $C_l$, knowledge of full spectral covariance matrix, or both!

As I mentioned above, the method is entirely blind, all non-zero parameters are free. For $\textbf{a}$ equals Planck channel (mixing matrix), $\textbf{F}$ foreground, we have 

$$
\textbf{d}_{lm} = [\textbf{a} \vert \textbf{F}] 
\begin{bmatrix} 
s_{lm}  \\ 
f_{lm}
\end{bmatrix} +\textbf{n}_{lm}
$$

then 

$$
\text{Cov}(\textbf{d}_{lm})= [\textbf{a} \vert \textbf{F}] 
\begin{bmatrix} 
C^{\text{cmb}}_{lm} & 0 \\ 
0 & P_l
\end{bmatrix}
[\textbf{a} \vert \textbf{F}]^{\dagger}
+ 
\begin{bmatrix}
\sigma^2_{al} & \cdots & 0 \\
\vdots & \ddots & \vdots \\
0 & \cdots & \sigma^2_{9l}
\end{bmatrix}
= \textbf{C}_l(\textbf{a}, \textbf{C}^{\text{cmb}}_l, \textbf{F}, \textbf{P}_l, \sigma^2_{il})
$$

for the 9 Planck channels.  

If no foreground combination can mimick the CMB angular spectrum, then the semi-parametric elements $\textbf{a} s_{lm}$ and $\textbf{F} \textbf{f}_{lm}$ are *uniquely* fitted. 


\textbf{BRAINSTORMING NOTES regarding anomalies}

Possibly not relevant, but if $C_l$ non-approximate, then many of these estimator discussions can be laid to rest. 

Note: some of these anomalies discussed today were dismissed with more data (e.g. Axis of Evil dismissed by authors after further WMAP data in 2009) and others were found to not exist so much in Plack (quadrupole, dipole alignment), and others are still being argued about. 

WMAP associated many of these anomalies with systematic effects which were corrected with 9-year data release 2012. 

According to Planck XXIII 2013, there are anomalies which remain:
\begin{itemize}

\item Doppler boositng pointing towards CMB dipole direction, which is well understood by special relativity. Significant at 3-4 sigma. 

\item Low-ell dipole modulation significant at 3 sigma. 

\item A hemispherical asymmetry at low multipoles but descreasing with ell. 

\item Significant high-ell asymmetry seen along direction $\approx60\degree$ away from dipole, appears as a mixture of two different effects and annot be explained in terms of just on or the other. Neither effect is highly significant on its own. 

\end{itemize}

Planck (Eriksen) do not find reality of asymmetry in power spectrum a "a posteriori statistic" or etc. This *isn't* just finding SH in the map. 

To make further progress, we need a single theoretical framework to jointly model the low-ell and high-ell regimes, using boosted corrected data. 

WHOLE POINT: However, many of these anomalies are thought to be proven/disproven with $C_l$ estimators. We try to shed light on this situation. 

\textbf{How $C_l$ used today}

These estimators used for all analyses. For instance, with tests for statistical anistropy in pure CMB signal maps:

Hanson, Lewis: 0908.0963
Hanson, Lewis,  Challinor: 1003.0198

\paragraph{} \hspace{0pt} \\


\textbf{Section discussing conflicts between $C_l$ estimators}

Example argument of pseudo-$C_l$ results are in disagreement with with maximum-likehood based $C_l$ results, using WMAPintroduced $S_{1/2}$ statistic. NOTE: The issue with these statistics is that they aren't really well-motivated, thus there's some a posterior bias involved.

See: 

C. L. Bennett et al. Seven-Year Wilkinson Microwave Anisotropy Probe (WMAP) Observations: Are There Cosmic Microwave Background Anomalies? 2010, 1001.4758. 16, 21, 27, 55

George Efstathiou, Yin-Zhe Ma, and Duncan Hanson. Large-Angle Correlations in the Cosmic Microwave Background. Mon. Not. Roy. Astron. Soc., 2010, 0911.5399. 16, 39

Andrew Pontzen and Hiranya V. Peiris. The cut-sky cosmic microwave background is not anomalous. Phys. Rev., D81:103008, 2010, 1004.2706. 16



$S_{1/2}$ statistic introduced in 2003 WMAP first year release, see section 7 "Intriguing Discrepancies", p. 24.  

D. N. Spergel et al. First Year Wilkinson Microwave Anisotropy Probe (WMAP) Observations: Determination of Cosmological Parameters. Astrophys. J. Suppl., 148:175–194, 2003, astro-ph/0302209. 1, 15, 16


Plotting the real-space angular correlation function $C(\theta)$, they found a lack of any correlated signal on angular scales greater than $60\degree$. So, they try to measure this possible effect by the following a posteriori statistic

$$
S=\int^{1/2}_{-1}[C(\theta)]^2\text{d}\cos\theta
$$

where upper and lower cutoffs deteremined a posterior after seeing correlation function shape.


By calculating \textbf{non-approximate power spectrum} with \textbf{exact covariance matrix}, you could possibly settle these sorts of debates. Just an idea.


*******

$C_l$ is the power spectrum. 
+
The covariance of $XX'$ is denoted by $C^{XX'}$, where

$$
C^{XX'}_{lm,l'm'}=<X_{lm}X^{'*}_{l'm'}>
$$

For statistically isotropic, the covariance is diagonal and completely described by the power spectrum $C^{XX'}_l$, which is defined by 

$$
C^{XX'}_{lm,l'm'}=\delta_{ll'}\delta_{mm'}C^{XX'}_{l}
$$

In order to distinguish between the CMB signal and the instrumentally-observed sky (which contains noise and effects of beam), we use overhat, $\hat{X}=X+N$, where $N$ is instrumental noise. 

So, for independent signal and noise, the covariance is denoted by $C^{\hat{X}\hat{X}}=C^{XX}+C^{NN}$. Note that $C^{\hat{X}\hat{X}}$ is an ensemble averaged quantity.



Likelihood function: 

For $N$ number of elements in CMB observation $\hat{d}$ data, we have a log-likelihood given by

$$
\mathcal{L}(\hat{d}|\theta)=-2 \ln P(\hat{d}|\theta)=\frac{1}{2}\hat{d}^{\dagger}
(C^{\hat{d}\hat{d}})^{-1}\hat{d}+\frac{1}{2}\ln\text{det}(C^{\hat{d}\hat{d}})+\frac{N}{2}\ln2\pi
$$

CORRECTION: no $1/2$ factors

where the parameters characterizing covariance is denoted by $\theta$, the covariance is $C^{\hat{d}\hat{d}}=C^{dd}+C^{NN}$, which equals theoretical covariance added to instrumental noise. 


The CMB community thinks "For modern datasets with hundreds of thousands of modes, the full covariance matrix is unmanageably large. To work with it directly therefore requires either artificial limitation of the analysis to some subset of the data or exploitable sparseness." This is a sample comment found in all papers. 

WE WOULD LIKE TO FIX THIS. 



The correlation function is 

$$
C(\theta_{ij})=\Sigma_l\frac{2l+1}{4\pi}C^{theortitcal}_l P_l(\cos\theta_{ij})
$$

Where $\theta_{ij}$ is the angle between the pixels. Note we put theoretical values of $C_l$ into the expression.

Alternatively, 

$$
C_l=4\pi\int d(\cos\theta)P_l(\cos\theta_{ij})C(\theta)
$$

Monopole $l=0$ (average temperature) or dipole $l=1$ (Doppler shift). 



Angle brackets denote average over an ensemble of realizations of fluctuations. If assume ideal, noise measurements of temperature over the FULL SKY, we use estimators. Problem: we have instrumental noise, noise which is heteroskedastic, i.e. not stationary/ uneven over different sky regions due to how measurements made/how many passes detector makes (i.e. different regions of the sky observed different times), and partial sky coverage/masked sky due to Galactic plane. This makes estimating the power spectrum complicated. Modleling beam effects is tricky as well, because maps at different channels have different resolutions with beams, beams not necessarily isotropic or spatially invariant. 

Further problems with power spectrum estimation: one never has fully sky coverage, and one should (SOMEHOW! SUBJECTIVELY?) give less weight to noisy pixels so as not to destroy information. That destroys orthogonality of spherical harmonics. 

\textbf{Why are Sky Cuts Bad/What do they do?}
Well, incomplete sky coverage increases sample variance by approx. $1/\sqrt{\text{Covered \ Area}}$ and smears out features in the power spectrum. 
With sky cuts, part of the signal is discarded along with contamination.

Note that at low ell, $l>50$ or so, uncertainities arise due to masked areas, primarily the Galactic plane. (There are also effects of "bad pixels" due to point sources, etc., but this mostly affects larger ells. However, Planck takes effects of point sources on low ells very seriously, see paper XV, 2013.)  There is no objective guide to how "clean" these pixels are though, beyond foreground removal techniques judged by chi-squared metric.)

See George Efstathiou for review of estimators. astro-ph/0307515
"Myths and Truths Concerning Estimation of Power Spectra", 2004
Discusses methods for power spectrum estimators and for deriving an accurate covariance matrix. States orthodox CMB community view that directly computing the power spectrum likelihood is computationally prohibitive/impossible. You either use strong assumptions or calculate approximate power spectrum. 

Typical statement: "In order to use the power spectrum for cosmological parameter estimation, you need to know the complete likehood function $P(d | C_l(\theta))$ where $\theta$ denotes cosmological parameters. So, power spectrum compution should give model spectra likehood function. PROBLEM: this evaluation is not computationally feasible at full-map resolution and hence there are different methods to calculate the likelihood function at low ell and high ell. Usually, the low ell algorithms use low resolution maps and do Bayesian inference. The high ell estimators use  frequentist estimators. 

As an example, some low-ell estimators use Gibbs sampling, i.e. samples from possible realization of the power spectrum likelihood given experimental data and then builds complete statistical description of each individual multipole component, $C_l$, of the power spectrum. 

For reference, let's introduce standard pseudo-power spectrum scheme MASTER from Eric Hivon, Krys. Gorski, Netterfield, et al, 2002, (all currently Planck people). "MASTER of the CMB anisotropy power spectrum: a fast method for statistical analysis of large and complex CMB data sets." arXiv:astro-ph/0105302
Also take a look at Ben Wandelt, Hivon, Gorski, "Pseudo Cl method", 2001, arXiv:astro-ph/0008111

NOTE: this method gives ell mode-mode coupling, which is induced by incomplete sky coverage/sky cuts/masked sky. WE WANT TO FIX THIS. 


Define pseuo-power spectrum 

$$
\widetilde{C}_l=\frac{1}{2l+1}\sum^{l}_{m=-l}\vert \tilde{a}_{lm}  \vert
$$

where degrees of freedom are $2l+1$. 

For fraction of the sky $f_{sky}$ and use a position-dependent weighting scheme $W(u)$ (which reduces edge effects or down-weight noisy pixels)

$$
f_{sky}=\frac{1}{4\pi}\int_{4\pi}d\textbf{u}W^{i}(\textbf{u})
$$

with $i$th moment of the weighting scheme. 

We can expand window function in spherical harmonic space, giving

$$
W_l=\frac{1}{2l+1}\sum_{m}\vert w_{lm}\vert^2
$$

And we can now apply window function $W(\textbf{u})$ to a temperature fluctuation map $\delta T(\textbf{u})$, decompose into pseudo-spherical harmonics $\tilde{a}_{lm}$, giving

$$
\tilde{a}_{lm}=\int d\textbf{u} \Delta T(\textbf{u})W(\textbf{u})Y^{*}_{lm}(\textbf{u}) \approx \Omega_p\sum_p  \Delta T(p)W(p)Y^{*}_{lm}(p)
$$

where $W(\Omega)$ is a position-dependent weighting function applied to the map, integration taken over whole sky approximated by summation over the pixels with surface area $\Omega_p$ of the CMP maps. 

We then relate the ensemble average of this spectrum to the full-sky angular power spectrum $C_l$ by 

$$
< \widetilde{C}_l > = \sum_{l'} M_{ll'} F_{l'} B^2_{l'}<C_{l'}> + <\widetilde{N}_l>
$$
where $M_{ll'}$ describes the effect of mode-mode coupling due to the sky cut, $B_l$ is a window function takng care of the smoothing effects of the beam and finite pixel size, $F_l$ is the transfer function modeling the filtering that is applied to the data/maps, and $<\widetilde{N}_l>$ is the average power spectrum of the noise, extracted from the actual data stream. Note $F_l$ is not motivated by theory, but is an ansatz that attempts to correct processing of TOD.  For $l>32$, this was used for WMAP. 

NOTE TO SELF ABOUT PIXELS/ANGULAR RESOLUTION CAPABILITY: COBE had 7 degree resolution, WMAP 15 arcmin resolution, Planck has 

Take COBE: full sky maps with an angular resolution $7\degree$, full sky has $4\pi$ $\text{radians}^2 \sim 41000$ $\text{degrees}^2$, so there are $840$ pixels with area of $(7\degree)^2$. Thus, the survey gives 840 independent pieces of information. If characterize this information with coefficients $a_{lm}$, then there is some $l_{\text{max}}$, above which there is no information. 

How to determine $l_{max}$: One way is to set total number of recoverable $a_{lm}$ as $\sum^{l_max}_{l=0}(2l+1)=(l_{max}+1)^2=840$. So, the information could be equally well-characterized by specifying all the $a_{lm}$ up to the $l_{max}=28$. 

So, COBE had an ell max of around 30. 

Distribution from which $a_{lm}$ is drawn has expectation of zero and width $C_l^{1/2}$. 

The mean value of all $a_{lm}$ is zero, and has nonzero variance. 

$$
<a_{lm}>=0, \ \ <a_{lm}a^{*}_{l'm'}>=\delta_{ll'}\delta_{mm'}C_l
$$

NOTE: For a given ell, each $a_{lm}$ has the same variance, e.g. for $l=100$, all 201 $a_{100,m}$s are drawn from the same distribution. Measuring the 201 coefficients is equivalent to sampling the distribution, and this information gives use idea of underlying variance of the distribution. (Exception ell around 2, which fundamentally constrained by cosmic variance.)


Papers to look at for computing power spectrum estimators:

Bond, Jaffe, Knox, "Estimating the Power Spectrum of the Cosmic Microwave Background"
arXiv:astro-ph/9903166
arXiv:astro-ph/9708203
Bond, Jaffe, Knox, "CMB Likelihood Functions for Beginners and Experts"  arXiv:astro-ph/0306506

PEIRIS DISSERTATION, CHAPTER 2, see astro-ph/0302218

They use MCMC to evaluate the likelihod function of model parameters, trying to find the parameters which give an ESTIMATE of $C_l$, 

Furthermore, she gives a good account of the affects of sky masks. WMAP masks bright sources and the Galactic plane. The unfortunate side effect is to couple multipole modes on the sky so that the power spectrum covariance matrix is no longer diagonal. Bullshit ensues. 




FEENEY, ways to work around sky cut, discussion: 

Maximum-likelihood estimators, $\hat{a}_{lm}$, are often used to reconstruct the large-scale spherical har- monic coefficients, alm, from partial-sky data. The technique relies on smoothing to restrict the amount of small-scale noise accessible to the reconstruction, but smoothing has been shown to contaminate “clean” pixels with residual foregrounds from within the sky cut. In this work, we have examined the impact of this smoothing-induced bias on the maximum-likelihood recon- struction. We have shown that it is possible to mitigate the bias by removing the contaminated regions, but these are only well-defined if smoothing is performed using a kernel with finite support on the sky. This precludes the use of the commonly used Gaussian kernel. Cutting a larger portion of the sky greatly increases the variance of the reconstruction, but it is possible to counteract this effect by enforcing a prior on the reconstructed coefficients using a Wiener filter. We have therefore proposed an estimator – using top-hat smoothing, extended masks and a Wiener-filtered reconstruction – which does not suffer from smoothing-induced bias. By considering the expectation of the square of the reconstruction error,$ Z = 􏰙l, m⟨(aˆlm − alm)2⟩$, we have compared the performance of the maximum-likelihood and Wiener-filtered estimators in the presence of simulated CMB foreground residuals.
The reconstruction performance measure Z scales with the estimators’ bias and variance, which in turn are governed by the amplitude of contamination and the size of the sky cut, respec- tively. The fiducial maximum-likelihood reconstruction is performed using relatively small sky cuts, but is susceptible to contamination through smoothing-induced bias; the finite-smoothing Wiener-filtered reconstruction does not suffer from smoothing-induced bias, but makes use of extended masks. Increasing the level of contamination therefore increases Z for the maximum- likelihood reconstruction only, which suggests that there is a level of contamination above which one should switch from the maximum-likelihood to the Wiener-filtered reconstruction.
Given an estimate of the morphology and amplitude of the contaminants within the cut sky, one can predict which modes will be biased and by how much, and hence determine the threshold
56
at which one should swap estimators. We find that this threshold is relatively insensitive to the precise morphology of foreground residuals at large scales, and is mainly governed by their am- plitude. Calculating Z for the two estimators in the presence of estimated foreground residuals, we determine this threshold to be ∼ 10 times the amplitude of the foreground residuals used in this work. Assuming that the ILC contains similar levels of contamination to those used here, we therefore recommend the use of either the contaminated full-sky alms or the fiducial maximum- likelihood aˆlms when handling this data-set. However, when using foreground-reduced maps for individual WMAP frequencies, which contain much greater foreground residuals, the Wiener- filtered reconstruction will provide the best estimate of the large-scale underlying CMB signal. Note that, as the Wiener-filtered aˆlms are a maximum-posterior solution, care must be taken if the reconstruction output is being used for further model-selection steps. The reconstruction techniques are, however, most commonly used to test the null hypothesis, in which case the prior employed in this work is completely appropriate.
For problems requiring only a power spectrum (as opposed to the full temperature field) the issues described in this paper are essentially irrelevant because the smoothing can be conducted on vastly smaller scales, the resulting range of poorly constrained modes being automatically downweighted.

our results of a non-approximate CMB likelihood should do more than affect next generation of surveys. It could have a large impact on current analyses:

Boris Leistedt

http://arxiv.org/pdf/1306.0005.pdf
http://arxiv.org/pdf/1404.6530.pdf  (featuring XDQSO from Hogg et. al)

Due to the incomplete data from sky cuts, they work with various $C_l$ estimators (which have restrictions and contrasting results).
Our method should improve this, especially regarding large scale/ low ell results in $C_l$. This *correctly* mitigates systematics (or at least, better than competing estimators.)

Peiris in particular has spent a good deal of time on similar issues i.e. grappling with partial sky coverage:
http://arxiv.org/pdf/1004.2706.pdf
http://arxiv.org/pdf/1107.5466.pdf

\paragraph{} \hspace{0pt} \\

\textbf{Further brainstorming ideas}
Planck sets weights/values to pixels. We suspect they average the TOD and set these values. Try to work with TOD and use sampling methods to improve this, and then carry out analysis. This all depends on how they set TOD to pixels, tractibility issues, etc. 

(For reference, see WMAP 1996 paper on computational method that allows for production of mega-pixel CMB anisotropy maps from large differential radiometer data sets. http://arxiv.org/pdf/astro-ph/9510102v1.pdf )

The effect of the map-making algo- rithm is to average the beam over the observed locations in a given pixel. This average is referred to as the effective beam, which will vary from pixel to pixel across the sky. (Planck VII, 2013). 


Description of how Planck sets pixel weights: 

Planck VI, 2013

Planck VII, 2013
Bolomerters, phonon-mediated thermal detectors with finite response time. 

Input of TOD approximated by complex Fourier domain transfer function, deconvolved by the time response function prior to calibration and map-making. Deconvolved TOIs represent true sky signal convolved by the optical response of the telescope (or physical beam) and filtered by TOI processing, i.e. flagging bad data. 

These deconvolved data are then projected into a pixelized map. To good approximation, the effect of the map-making algorithm is to average the beam over the observed locations in a given pixel. This average is referred to as the effective beam, which will vary from pixel to pixel across the sky. 

Overview of the measurement of the Planck beams and the resulting harmonic-space window functions that describe the net optical and electronic response to the sky signal. At any given time, the response to a point source is given by the combination of the optical response of the Planck telescope (i.e. the optical beam) and the electronic transfer function. The response pattern is the "scanning beam", i.e. 2D elliptical Gaussian with small perturbations at a level of a few percent of the peak. Scanning beams are used to calculate the effective beam response at a given pixel.  

According to the mapmaking procedure, any map pixels is the sum of the many different elements of the timeline, each of which has hit the pixel in a different location and different direction. Therefore, the effective beam takes into account the details of the scan pattern. The multiplicative effect on the angular power spectrum is encoded in the "effective beam window function", which include the appropriate weights for analyzing aggregate maps across detector sets or frequencies. 

HFI mapmaking: the inputs for each detector come from TOI processing with associated invalid data flags. Take advantage of the redundancies during a stable pointing period (ring of data) by averaging each detector's measurements in HEALPix pixels, building a new data structure, the HEALPix pixel ring. We use this for mapmaking instead of phased binned rings, because that would introduce unwanted smoothing. 

The effective beam is the angular response including the effect of the optics, detectors, data processing and the scan strategy. The window function is the representation of this beam in the harmonic domain that is required to recover an unbiased measurement of the CMB angular power spectrum.

To average the beam over the observed locations in a given pixel. This average is referred to as the effective beam, which will vary from pixel to pixel across the sky. The map- making procedure implicitly ignores any smearing of the input TOIs; no attempt is made to deconvolve the optical beam and any remaining electronic time response. Thus, any further use of the resulting maps must take into account the effective beam.

Planck VIII, 2013

The first step takes advantage of the redundancy of the observations on the sky. For each detector, we average the measurements in each HEALPix pixel visited during a stable pointing period (hereafter called ring). The subsequent calibration and mapmaking operations use this intermediate product, called HPR for HEALPix Pixels Ring, as input. As we produce HEALPix maps with the resolution pa- rameter Nside set to 2048 we use the same internal resolution for building the HPR.

Using the averaged hit count per pixel, we convert these averages into an equivalent r.m.s per TOI sample. 



\paragraph{} \hspace{0pt} \\



\textbf{Notes on why data is heteroscedastic}

See "Artifacts near caustics of the scanning strategy" at \\ $http://wiki.cosmos.esa.int/planckpla/index.php/Frequency_Maps$

"The scanning strategy is such that regions around the Ecliptic poles are surveyed very deeply and compared to the average, and the transition from the nominal depth to the high depth, as shows on hit-count maps is very rapid, namely a few pixels, for a contrast of ~30. These transitions, or caustics in the maps, occur at different positions on the sky for different detectors, as the positions depend on their location in the focal plane of the instrument. As a result, when data from different detectors are combined to build a full channel map, the the weights of different detectors in the mix changes rapidly across the caustic, and given the remaining errors in the relative calibration of the detectors, a visible effect can be introduced in the maps, especially when the SNR is very high, i.e. at the highest frequencies and near bright regions like the Galactic Plane."



\textbf{Future ideas for improvement}

Write down ideas to improve extraction algorithms for PS and SZ cluster catalogues. 



\textbf{HEALPix grid}

see astro-ph/0409513


A HEALPix map has $N_{\text{pix}}=12N^2_{\text{side}}$ pixels of the same area $\Omega_{\text{pix}}=\frac{\pi}{3N^2_{\text{side}}}$. 

All pixel centers are located on $N_{\text{ring}}=4\times N_{\text{side}}-1$ rings of constant latittude. 

Recall there are two different pixel indexing schemes, RING (moving from North to South along each isolatitude ring) or NESTED (arrange the pixel indices in a tree structure). RING is the scheme to use if you wish to use Fourier transforms with spherical harmonics. 

For $N_{\text{side}}=1024$, $\theta_{\text{pix}}=\Omega_{\text{pix}}^{1/2}=3.44^{'}$. 

And for $N_{\text{side}}=2048$, $\theta_{\text{pix}}=\Omega_{\text{pix}}^{1/2}=1.72^{'}$. 
%p.14 HEALPix arxiv

Note angular resolution is defined as 

$$
\theta_{\text{pix}}\equiv\sqrt{\Omega_{\text{pix}}}=\sqrt{\frac{3}{\pi}}\frac{3600}{1'}\frac{1}{N_{\text{side}}}
$$

Tesselation schemes and equations use $N_{\theta}$, the number of base-resolution pixel layers between the north and the south poles, and $N_{\phi}$, the number of equatorial pixel layers. The number of base resolution pixels equal to $N_{\text{pix}}=N_{\theta}\times N_{\phi}$. 

PIXEL POSITIONS 
Locations on the sphere defined by $(z\equiv\cos\theta, \phi)$ wihere $\theta\in[0,\pi]$ is the colatitude in radians measured from North Pole and $\phi\in[0,2\pi]$ is the longitude in radians measured Eastward. 

\textbf{Question: What affects does this have on cosmological parameter estimation, if anything? How would you write a transfer function and etc. in real space?}

Recall (Durrer p.210) Given observed spectrum can be obtained by a nearly arbitrary choice of cosmological parameters. For a given initial power spectrum $P_m(k)$ of scalar $(m=0)$, vector $(m=\pm1)$ and tensor $(m=\pm2)$ perturbations, under the assumption of statistical homogeneity and isotropy, the resulting CMB power spectrum is generally of the form

$$
C_l=\sum^2_{m=-2}\int\text{d}k \ T_m(l,k)P_m(k)
$$

where for a nearly arbitrary transfer function $T_m(l,k)$ and arbitrary $C_l$ one can find initial power spectra $P_m$ such that the equation holds. Recall we are not really "measuring" cosmological parameters, but "estimating" them under certain well-motivated but very restrictive assumptions on the initial power spectrum.


\textbf{Question: Could we possibly do something similar with TE cross-correlation? Probably not, but it's worth a thought. Not sure how to decompose E and B into real space.}

Method to extract CMB polarization power spectra via two-point correlation functions.
http://arxiv.org/pdf/astro-ph/0303414.pdf

""
The accurate analysis of CMB data places strong demands on the statistical methods employed. Even for total intensity data, which is simpler than polarization, the extraction of the power spectrum from upcoming mega- pixel datasets with standard maximum-likelihood methods (e.g. Bond, Jaffe, Knox 1998) is beyond the range of any supercomputer. [The operations count scales as the number ofpixelscubed,Np3ix,whilethestoragerequirementsare O(Np2ix).] In the search for fast alternatives to brute-force maximum-likelihood power spectrum estimation, two broad approaches have emerged. In the first, experiment-specific symmetries are exploited to make the brute-force analysis tractable, or, if the symmetries are only approximate, to pre-condition an iterative solution to the likelihood max- imisation. An example of the former is the ingenious “ring- torus” method of Wandelt , Hansen (2003), while the lat- ter was pioneered by Oh, Spergel , Hinshaw (1999) during the development of the pipeline for the WMAP satellite. The second class of methods sacrifice optimality in favour of speed by adopting a more heuristic weighting of the data (such as inverse weighting with the noise variance). An estimate of the underlying power spectrum is then obtained from the raw, rotationally-invariant power spectrum of the weighted map (the pseudo-Cls; Wandelt, Hivon , Gorski 2001) either by a direct linear inversion (Szapudi, Prunet , Colombi 2001; Hivon et al. 2002) or with likelihood methods (Wandelt et al. 2001; Hansen, G ́orski , Hivon 2002). The linear inversion, which yields estimators quadratic in the data, can be performed directly in harmonic space (Hivon et al. 2002), or, more simply, by first transforming to real space (i.e. by constructing the correlation function) and then recovering the power spectrum with a (suitably apodized) integral transform (Szapudi et al. 2001). The estimation of polarization power spectra is less well explored than for total intensity, although all of the above methods can, in principle, be extended to handle polarization. To date, only brute-force maximum likelihood (Kovac et al. 2002, Munshi et al. in preparation), minimum-variance quadratic estima- tors (Tegmark , de Oliveira-Costa 2002), pseudo-Cl meth- ods with statistical (Hansen , G ́orski 2003) or direct inver- sion (Kogut et al. 2003) in harmonic space, and real-space correlation function methods (Sbarra et al. 2003) have been demonstrated on polarized data. Of these, only the pseudo- Cl methods are fast enough to apply many times (e.g. in Monte-Carlo simulations) to mega-pixel maps. In this paper we extend the fast correlation-function approach of Szapudi et al. (2001) to polarized data.
""

Another reason why these $C_l$ estimators are such a problem: Duncan Hanson, arXiv:1003.0918
Recall we observe the actual CMB signal through a convolution of an instrumental beam. If these beams are asymmetric, they can introduce statistical anisotropies into the observed CMB, i.e. they can *bias* estimator. THIS IS A BIT IFFY. SEE SUMMRY OF CMB SCAN STRATEGY THOUGH. 

\textbf{Note} %Gorski http://www.das.inpe.br/school/AdvancedCourse/pdfs/Gorski_lecture.pdf
$$
\langle a_{lm}a_{l'm'} \rangle = C_l \delta_{ll'}\delta_{mm'}
$$

The sum of $m$'s is due to statistical isotropy, and the delta functions designate rotational invariance. 


\end{document}